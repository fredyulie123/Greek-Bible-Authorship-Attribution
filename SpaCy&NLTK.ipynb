{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np \n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer #tokenizes and counts words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #alternatively, use TfidfTransformer()\n",
    "from sklearn import metrics\n",
    "import requests\n",
    "from sklearn.feature_selection import RFE, chi2\n",
    "from scipy.spatial import distance\n",
    "from pandas.core.frame import DataFrame\n",
    "from nltk import FreqDist\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.matutils import kullback_leibler, jaccard, hellinger, sparse2full\n",
    "import string\n",
    "from scipy.stats import entropy\n",
    "from numpy.linalg import norm\n",
    "from scipy.spatial import distance\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import math\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.lang.es import Spanish\n",
    "from spacy.lang.el import Greek\n",
    "from spacy.lang.zh import Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice(im1, im2):\n",
    "    \"\"\"\n",
    "    Computes the Dice coefficient, a measure of set similarity.\n",
    "    Parameters\n",
    "    ----------\n",
    "    im1 : array-like, bool\n",
    "        Any array of arbitrary size. If not boolean, will be converted.\n",
    "    im2 : array-like, bool\n",
    "        Any other array of identical size. If not boolean, will be converted.\n",
    "    Returns\n",
    "    -------\n",
    "    dice : float\n",
    "        Dice coefficient as a float on range [0,1].\n",
    "        Maximum similarity = 1\n",
    "        No similarity = 0\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    The order of inputs for `dice` is irrelevant. The result will be\n",
    "    identical if `im1` and `im2` are switched.\n",
    "    \"\"\"\n",
    "    im1 = np.asarray(im1).astype(np.bool)\n",
    "    im2 = np.asarray(im2).astype(np.bool)\n",
    "\n",
    "    if im1.shape != im2.shape:\n",
    "        raise ValueError(\"Shape mismatch: im1 and im2 must have the same shape.\")\n",
    "\n",
    "    # Compute Dice coefficient\n",
    "    intersection = np.logical_and(im1, im2)\n",
    "\n",
    "    return 2. * intersection.sum() / (im1.sum() + im2.sum())                                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True False  True  True]\n"
     ]
    }
   ],
   "source": [
    "im1 = [3,4,0,5,3]\n",
    "im2 = [0,3,4,5,2]\n",
    "im1 = np.asarray(im1).astype(np.bool)\n",
    "im2 = np.asarray(im2).astype(np.bool)\n",
    "intersection = np.logical_and(im1, im2)\n",
    "print(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True, False,  True,  True])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('NA28_11_09_11PM.csv', encoding = 'utf-16', sep = '\\t')\n",
    "df2 = pd.read_csv('SBL_11_09_11PM.csv', encoding = 'utf-16', sep = '\\t')\n",
    "df3 = pd.read_csv('Tyndale_11_09_11PM.csv', encoding = 'utf-16', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_text(text):\n",
    "    text_list = []\n",
    "    for t in text['Verse']:\n",
    "        text_list.append(t)\n",
    "\n",
    "    text_str = ' '.join(text_list)\n",
    "    return text_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_list(text):\n",
    "    spacy_stopwords = spacy.lang.el.stop_words.STOP_WORDS\n",
    "    # spacy_stopwords is a hardcoded set\n",
    "    nlp = Greek()\n",
    "    tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
    "    tokens = tokenizer(text)\n",
    "    tokens1 = tokenizer(hbr)\n",
    "    #token_list = []\n",
    "    lemma_list = []\n",
    "    lemma_list1 = []\n",
    "    for token in tokens:\n",
    "        if token.lemma_.lower() not in spacy_stopwords:\n",
    "        #token_list.append(token.text)\n",
    "            lemma_list.append(token.lemma_)\n",
    "    for token in tokens1:\n",
    "        if token.lemma_.lower() not in spacy_stopwords:\n",
    "        #token_list.append(token.text)\n",
    "            lemma_list1.append(token.lemma_)\n",
    "    #token_list\n",
    "    dict1 = {}\n",
    "    dict2 = {}\n",
    "    for key in lemma_list:\n",
    "        dict1[key] = dict1.get(key, 0) + 1\n",
    "    for key in lemma_list1:\n",
    "        dict2[key] = dict2.get(key, 0) + 1\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "    for word in dict1:\n",
    "        if word not in dict2:\n",
    "            l2.append(word)\n",
    "    for word in dict2:\n",
    "        if word not in dict1:\n",
    "            l1.append(word)\n",
    "    for x in l1:\n",
    "        dict1[x] = dict1.get(x,0)\n",
    "    for y in l2:\n",
    "        dict2[y] = dict2.get(y,0)\n",
    "    s_dict1 = sorted(dict1.items())\n",
    "    s_dict2 = sorted(dict2.items())\n",
    "    m1 = []\n",
    "    m2 = []\n",
    "    for m in s_dict1:\n",
    "        m1.append(m[1])\n",
    "    for n in s_dict2:\n",
    "        m2.append(n[1])\n",
    "    return [m1,m2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_list1(text):\n",
    "    spacy_stopwords = spacy.lang.el.stop_words.STOP_WORDS\n",
    "    # spacy_stopwords is a hardcoded set\n",
    "    nlp = Greek()\n",
    "    tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
    "    tokens = tokenizer(text)\n",
    "    tokens1 = tokenizer(hbr)\n",
    "    #token_list = []\n",
    "    lemma_list = []\n",
    "    lemma_list1 = []\n",
    "    for token in tokens:\n",
    "        if token.lemma_.lower() not in spacy_stopwords:\n",
    "        #token_list.append(token.text)\n",
    "            lemma_list.append(token.lemma_)\n",
    "    for token in tokens1:\n",
    "        if token.lemma_.lower() not in spacy_stopwords:\n",
    "        #token_list.append(token.text)\n",
    "            lemma_list1.append(token.lemma_)\n",
    "    #token_list\n",
    "    dict1 = {}\n",
    "    dict2 = {}\n",
    "    for key in lemma_list:\n",
    "        dict1[key] = dict1.get(key, 0) + 1\n",
    "    for key in lemma_list1:\n",
    "        dict2[key] = dict2.get(key, 0) + 1\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "    for word in dict1:\n",
    "        if word not in dict2:\n",
    "            l2.append(word)\n",
    "    for word in dict2:\n",
    "        if word not in dict1:\n",
    "            l1.append(word)\n",
    "    for x in l1:\n",
    "        dict1[x] = dict1.get(x,0)\n",
    "    for y in l2:\n",
    "        dict2[y] = dict2.get(y,0)\n",
    "    s_dict1 = sorted(dict1.items())\n",
    "    s_dict2 = sorted(dict2.items())\n",
    "    m1 = []\n",
    "    m2 = []\n",
    "    for m in s_dict1:\n",
    "        m1.append(m[1])\n",
    "    for n in s_dict2:\n",
    "        m2.append(n[1])\n",
    "    return [m1,m2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NA28_Version_Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Mt', 'Mk', 'Lk', 'Joh', 'Apg', 'RÃ¶m', '1.Kor', '2.Kor', 'Gal',\n",
       "       'Eph', 'Phil', 'Kol', '1.Thess', '2.Thess', '1.Tim', '2.Tim',\n",
       "       'Tit', 'Phlm', 'heb', 'Jak', '1.Petr', '2.Petr', '1.Joh', '2.Joh',\n",
       "       '3.Joh', 'Jud', 'Offb'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.BookName.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matthew = df[df.BookName == 'Mt']\n",
    "Matthew['Author'] = 'Matthew'\n",
    "Mark = df[df.BookName == 'Mk']\n",
    "Mark['Author'] = 'Mark'\n",
    "Luke = df[df.BookName == 'Lk']\n",
    "Luke['Author'] = 'Luke'\n",
    "John = df[df.BookName == 'Joh']\n",
    "John['Author'] = 'John'\n",
    "Acts = df[df.BookName == 'Apg']\n",
    "Acts['Author'] = 'Luke'\n",
    "Luke_Book = pd.concat([Luke,Acts],axis = 0)\n",
    "James = df[df.BookName == 'Jak']\n",
    "James['Author'] = 'James'\n",
    "f_Peter = df[df.BookName == '1.Petr']\n",
    "s_Peter = df[df.BookName == '2.Petr']\n",
    "Peter_letter = pd.concat([f_Peter, s_Peter], axis = 0)\n",
    "Peter_letter['Author'] = 'Peter'\n",
    "f_John = df[df.BookName == '1.Joh']\n",
    "s_John = df[df.BookName == '2.Joh']\n",
    "t_John = df[df.BookName == '3.Joh']\n",
    "John_letter = pd.concat([f_John, s_John, t_John], axis = 0)\n",
    "John_letter['Author'] = 'John'\n",
    "Jude = df[df.BookName == 'Jud']\n",
    "Jude['Author'] = 'Jude'\n",
    "Rev = df[df.BookName == 'Offb']\n",
    "Rev['Author'] = 'John'\n",
    "John_Book = pd.concat([John,John_letter,Rev],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Romans = df[df.BookName == 'RÃ¶m']\n",
    "Romans['Author'] = 'Paul'\n",
    "f_cor = df[df.BookName == '1.Kor']\n",
    "f_cor['Author'] = 'Paul'\n",
    "s_cor = df[df.BookName == '2.Kor']\n",
    "s_cor['Author'] = 'Paul'\n",
    "Gal = df[df.BookName == 'Gal']\n",
    "Gal['Author'] = 'Paul'\n",
    "Eph = df[df.BookName == 'Eph']\n",
    "Eph['Author'] = 'Paul'\n",
    "Phi = df[df.BookName == 'Phil']\n",
    "Phi['Author'] = 'Paul'\n",
    "Col = df[df.BookName == 'Kol']\n",
    "Col['Author'] = 'Paul'\n",
    "f_the = df[df.BookName == '1.Thess']\n",
    "f_the['Author'] = 'Paul'\n",
    "s_the = df[df.BookName == '2.Thess']\n",
    "s_the['Author'] = 'Paul'\n",
    "f_tim = df[df.BookName == '1.Tim']\n",
    "f_tim['Author'] = 'Paul'\n",
    "s_tim = df[df.BookName == '2.Tim']\n",
    "s_tim['Author'] = 'Paul'\n",
    "Titus = df[df.BookName == 'Tit']\n",
    "Titus['Author'] = 'Paul'\n",
    "Philemon = df[df.BookName == 'Phlm']\n",
    "Philemon['Author'] = 'Paul'\n",
    "Paul_letters = pd.concat([Romans,f_cor,s_cor,Gal,Eph,Phi,Col,f_the,s_the,f_tim,s_tim,Titus,Philemon],axis = 0)\n",
    "Heb = df[df.BookName == 'heb']\n",
    "Heb['Author'] = 'Paul?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = cosine_text(Matthew)\n",
    "g2 = cosine_text(Mark)\n",
    "g3 = cosine_text(Luke)\n",
    "g4 = cosine_text(John)\n",
    "acts = cosine_text(Acts)\n",
    "rms = cosine_text(Romans)\n",
    "fcor = cosine_text(f_cor)\n",
    "scor = cosine_text(s_cor)\n",
    "gal = cosine_text(Gal)\n",
    "eph = cosine_text(Eph)\n",
    "phi = cosine_text(Phi)\n",
    "col = cosine_text(Col)\n",
    "fthe = cosine_text(f_the)\n",
    "sthe = cosine_text(s_the)\n",
    "ftim = cosine_text(f_tim)\n",
    "stim = cosine_text(s_tim)\n",
    "titus = cosine_text(Titus)\n",
    "philemon = cosine_text(Philemon)\n",
    "jams = cosine_text(James)\n",
    "fpet = cosine_text(f_Peter)\n",
    "spet = cosine_text(s_Peter)\n",
    "fjohn = cosine_text(f_John)\n",
    "sjohn = cosine_text(s_John)\n",
    "tjohn = cosine_text(t_John.dropna())\n",
    "jud = cosine_text(Jude)\n",
    "rev = cosine_text(Rev)\n",
    "hbr = cosine_text(Heb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "g11 = word_list(g1)\n",
    "g21 = word_list(g2)\n",
    "g31 = word_list(g3)\n",
    "g41 = word_list(g4)\n",
    "acts1 = word_list(acts)\n",
    "rms1 = word_list(rms)\n",
    "fcor1 = word_list(fcor)\n",
    "scor1 = word_list(scor)\n",
    "gal1 = word_list(gal)\n",
    "eph1 = word_list(eph)\n",
    "phi1 = word_list(phi)\n",
    "col1 = word_list(col)\n",
    "fthe1 = word_list(fthe)\n",
    "sthe1 = word_list(sthe)\n",
    "ftim1 = word_list(ftim)\n",
    "stim1 = word_list(stim)\n",
    "titus1 = word_list(titus)\n",
    "philemon1 = word_list(philemon)\n",
    "jams1 = word_list(jams)\n",
    "fpet1 = word_list(fpet)\n",
    "spet1 = word_list(spet)\n",
    "fjohn1 = word_list(fjohn)\n",
    "sjohn1 = word_list(sjohn)\n",
    "tjohn1 = word_list(tjohn)\n",
    "jud1 = word_list(jud)\n",
    "rev1 = word_list(rev)\n",
    "hbr1 = word_list(hbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [g11,g21,g31,g41,acts1,rms1,fcor1,scor1,gal1,eph1,phi1,col1,fthe1,sthe1,ftim1,stim1,titus1,philemon1,jams1,fpet1,spet1,fjohn1,sjohn1,tjohn1,jud1,rev1]\n",
    "s_dice = []\n",
    "s_hel = []\n",
    "s_jac = []\n",
    "s_jen = []\n",
    "a = []\n",
    "for x in range(0,len(documents)):\n",
    "    a.append(distance.dice(documents[x][0],documents[x][1]))\n",
    "    s_dice.append(dice(documents[x][0],documents[x][1]))\n",
    "    s_hel.append(hellinger(documents[x][0],documents[x][1]))\n",
    "    s_jac.append(jaccard(documents[x][0],documents[x][1]))\n",
    "    s_jen.append(distance.jensenshannon(documents[x][0],documents[x][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.19900826446280992,\n",
       " 0.20012404382881951,\n",
       " 0.1875,\n",
       " 0.20788979336255478,\n",
       " 0.1865581675708258,\n",
       " 0.23455166973442018,\n",
       " 0.21860701576004068,\n",
       " 0.2206060606060606,\n",
       " 0.20620842572062084,\n",
       " 0.2030608435983576,\n",
       " 0.18692338547934215,\n",
       " 0.1772875816993464,\n",
       " 0.18440905280804695,\n",
       " 0.16315307057745188,\n",
       " 0.17484662576687116,\n",
       " 0.16892168921689216,\n",
       " 0.11360513055428309,\n",
       " 0.09566968781470292,\n",
       " 0.1831888428194497,\n",
       " 0.18737749901999215,\n",
       " 0.17848206839032527,\n",
       " 0.18119364534134821,\n",
       " 0.08251676121712223,\n",
       " 0.07632800412583807,\n",
       " 0.12182254196642686,\n",
       " 0.21287379624936645]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('el_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_cosine = []\n",
    "documents = [g1,g2,g3,g4,acts,rms,fcor,scor,gal,eph,phi,col,fthe,sthe,ftim,stim,titus,philemon,jams,fpet,spet,fjohn,sjohn,tjohn,jud,rev]\n",
    "bindex = list(range(0,27))\n",
    "doc1 = nlp(hbr)\n",
    "for x in range(0,len(documents)):\n",
    "    doc2 = nlp(documents[x])\n",
    "    similarity = doc1.similarity(doc2)\n",
    "    s_cosine.append(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cosine = []\n",
    "documents = [hbr,g1,g2,g3,g4,acts,rms,fcor,scor,gal,eph,phi,col,fthe,sthe,ftim,stim,titus,philemon,jams,fpet,spet,fjohn,sjohn,tjohn,jud,rev]\n",
    "bindex = list(range(0,27))\n",
    "for x in range(1,len(documents)):\n",
    "    two_books = [hbr,documents[x]]\n",
    "    # Create the Document Term Matrix\n",
    "    count_vectorizer = CountVectorizer(stop_words='greek')\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    sparse_matrix = count_vectorizer.fit_transform(two_books)\n",
    "\n",
    "    # Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    df_m5 = pd.DataFrame(doc_term_matrix, \n",
    "                      columns=count_vectorizer.get_feature_names(), \n",
    "                      index=['Hebrews','Compared Book'])\n",
    "    # df_m5.to_csv(str(bindex[x])+'compared Hebrews term weight matrix.csv', encoding = 'utf-16',sep = '\\t', index=False)\n",
    "    cos = tuple(cosine_similarity(df_m5)[0])[1]\n",
    "    n_cosine.append(cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dice, js divergence, hellinger distance\n",
    "n_dice = []\n",
    "n_hel = []\n",
    "n_jac = []\n",
    "n_jen = []\n",
    "documents = [hbr,g1,g2,g3,g4,acts,rms,fcor,scor,gal,eph,phi,col,fthe,sthe,ftim,stim,titus,philemon,jams,fpet,spet,fjohn,sjohn,tjohn,jud,rev]\n",
    "bindex = list(range(0,len(documents)))\n",
    "for x in range(1,len(documents)):\n",
    "    two_books = [hbr,documents[x]]\n",
    "    count_vectorizer = CountVectorizer(stop_words='greek')\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    sparse_matrix = count_vectorizer.fit_transform(two_books)\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    m1 = doc_term_matrix[0].tolist()\n",
    "    m2 = doc_term_matrix[1].tolist()\n",
    "    n_dice.append(dice(m1[0],m2[0]))\n",
    "    n_hel.append(hellinger(m1[0],m2[0]))\n",
    "    n_jac.append(jaccard(m1[0],m2[0]))\n",
    "    n_jen.append(distance.jensenshannon(m1[0],m2[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "NA28 = {'SpacyCosine':s_cosine, 'SpacyDice':s_dice, 'SpacyHellinger':s_hel, 'SpacyJaccard':s_jac, 'SpacyJSdivergence':s_jen, 'NLTKCosine':n_cosine, 'NLTKDice':n_dice, 'NLTKHellinger':n_hel, 'NLTKJaccard':n_jac, 'NLTKJSdivergence':n_jen}\n",
    "NA28 = pd.DataFrame(NA28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_s1 = cosine_text(Matthew)\n",
    "mark_s1 = cosine_text(Mark)\n",
    "luke_s1 = cosine_text(Luke_Book)\n",
    "jam_s1 = cosine_text(James)\n",
    "pet_s1 = cosine_text(Peter_letter)\n",
    "john_s1 = cosine_text(John_Book.dropna())\n",
    "jude_s1 = cosine_text(Jude)\n",
    "pl_s1 = cosine_text(Paul_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_s2 = word_list(mat_s1)\n",
    "mark_s2 = word_list(mark_s1)\n",
    "luke_s2 = word_list(luke_s1)\n",
    "jam_s2 = word_list(jam_s1)\n",
    "pet_s2 = word_list(pet_s1)\n",
    "john_s2 = word_list(john_s1)\n",
    "jude_s2 = word_list(jude_s1)\n",
    "pl_s2 = word_list(pl_s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_dice1 = []\n",
    "s_hel1 = []\n",
    "s_jac1 = []\n",
    "s_jen1 = []\n",
    "documents = [pl_s2,mat_s2,mark_s2,luke_s2,jam_s2,pet_s2,john_s2,jude_s2]\n",
    "for x in range(0,len(documents)):\n",
    "    s_dice1.append(dice(documents[x][0],documents[x][1]))\n",
    "    s_hel1.append(hellinger(documents[x][0],documents[x][1]))\n",
    "    s_jac1.append(jaccard(documents[x][0],documents[x][1]))\n",
    "    s_jen1.append(distance.jensenshannon(documents[x][0],documents[x][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_cosine1 = []\n",
    "documents = [pl_s1,mat_s1,mark_s1,luke_s1,jam_s1,pet_s1,john_s1,jude_s1]\n",
    "authorindex = list(range(0,len(documents)))\n",
    "doc1 = nlp(hbr)\n",
    "for x in range(0,len(documents)):\n",
    "    doc2 = nlp(documents[x])\n",
    "    similarity = doc1.similarity(doc2)\n",
    "    s_cosine1.append(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cosine1 = []\n",
    "documents = [pl_s1,mat_s1,mark_s1,luke_s1,jam_s1,pet_s1,john_s1,jude_s1]\n",
    "authorindex = list(range(0,len(documents)))\n",
    "for x in range(0,len(documents)):\n",
    "    two_books = [hbr,documents[x]]\n",
    "    # Create the Document Term Matrix\n",
    "    count_vectorizer = CountVectorizer(stop_words='greek')\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    sparse_matrix = count_vectorizer.fit_transform(two_books)\n",
    "\n",
    "    # Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    df_m6 = pd.DataFrame(doc_term_matrix, \n",
    "                      columns=count_vectorizer.get_feature_names(), \n",
    "                      index=['Paul','compared books'])\n",
    "    # df_m6.to_csv(str(authorindex[x])+'author compare with Paul term weight matrix.csv', encoding = 'utf-8', index=False)\n",
    "    cos = tuple(cosine_similarity(df_m6)[0])[1]\n",
    "    n_cosine1.append(cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_dice1 = []\n",
    "n_hel1 = []\n",
    "n_jac1 = []\n",
    "n_jen1 = []\n",
    "documents = [pl_s1,mat_s1,mark_s1,luke_s1,jam_s1,pet_s1,john_s1,jude_s1]\n",
    "authorindex = list(range(0,len(documents)))\n",
    "for x in range(0,len(documents)):\n",
    "    two_books = [hbr,documents[x]]\n",
    "    count_vectorizer = CountVectorizer(stop_words='greek')\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    sparse_matrix = count_vectorizer.fit_transform(two_books)\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    m1 = doc_term_matrix[0].tolist()\n",
    "    m2 = doc_term_matrix[1].tolist()\n",
    "    n_dice1.append(dice(m1[0],m2[0]))\n",
    "    n_hel1.append(hellinger(m1[0],m2[0]))\n",
    "    n_jac1.append(jaccard(m1[0],m2[0]))\n",
    "    n_jen1.append(distance.jensenshannon(m1[0],m2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "NA281 = {'SpacyCosine':s_cosine1, 'SpacyDice':s_dice1, 'SpacyHellinger':s_hel1, 'SpacyJaccard':s_jac1, 'SpacyJSdivergence':s_jen1, 'NLTKCosine':n_cosine1, 'NLTKDice':n_dice1, 'NLTKHellinger':n_hel1, 'NLTKJaccard':n_jac1, 'NLTKJSdivergence':n_jen1}\n",
    "NA281 = pd.DataFrame(NA281)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_s4 = word_list1(mat_s1)\n",
    "mark_s4 = word_list1(mark_s1)\n",
    "luke_s4 = word_list1(luke_s1)\n",
    "jam_s4 = word_list1(jam_s1)\n",
    "pet_s4 = word_list1(pet_s1)\n",
    "john_s4 = word_list1(john_s1)\n",
    "jude_s4 = word_list1(jude_s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_dice2 = []\n",
    "s_hel2 = []\n",
    "s_jac2 = []\n",
    "s_jen2 = []\n",
    "documents = [mat_s4,mark_s4,luke_s4,jam_s4,pet_s4,john_s4,jude_s4]\n",
    "for x in range(0,len(documents)):\n",
    "    s_dice2.append(dice(documents[x][0],documents[x][1]))\n",
    "    s_hel2.append(hellinger(documents[x][0],documents[x][1]))\n",
    "    s_jac2.append(jaccard(documents[x][0],documents[x][1]))\n",
    "    s_jen2.append(distance.jensenshannon(documents[x][0],documents[x][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_cosine2 = []\n",
    "documents = [mat_s1,mark_s1,luke_s1,jam_s1,pet_s1,john_s1,jude_s1]\n",
    "authorindex = list(range(0,len(documents)))\n",
    "doc1 = nlp(pl_s1)\n",
    "for x in range(0,len(documents)):\n",
    "    doc2 = nlp(documents[x])\n",
    "    similarity = doc1.similarity(doc2)\n",
    "    s_cosine2.append(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cosine2 = []\n",
    "documents = [mat_s1,mark_s1,luke_s1,jam_s1,pet_s1,john_s1,jude_s1]\n",
    "authorindex = list(range(0,len(documents)))\n",
    "for x in range(0,len(documents)):\n",
    "    two_books = [pl_s1,documents[x]]\n",
    "    # Create the Document Term Matrix\n",
    "    count_vectorizer = CountVectorizer(stop_words='greek')\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    sparse_matrix = count_vectorizer.fit_transform(two_books)\n",
    "\n",
    "    # Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    df_m7 = pd.DataFrame(doc_term_matrix, \n",
    "                      columns=count_vectorizer.get_feature_names(), \n",
    "                      index=['Paul','compared books'])\n",
    "    # df_m6.to_csv(str(authorindex[x])+'author compare with Paul term weight matrix.csv', encoding = 'utf-8', index=False)\n",
    "    cos = tuple(cosine_similarity(df_m7)[0])[1]\n",
    "    n_cosine2.append(cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dice2 = []\n",
    "n_hel2 = []\n",
    "n_jac2 = []\n",
    "n_jen2 = []\n",
    "documents = [mat_s1,mark_s1,luke_s1,jam_s1,pet_s1,john_s1,jude_s1]\n",
    "authorindex = list(range(0,len(documents)))\n",
    "for x in range(0,len(documents)):\n",
    "    two_books = [pl_s1,documents[x]]\n",
    "    count_vectorizer = CountVectorizer(stop_words='greek')\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    sparse_matrix = count_vectorizer.fit_transform(two_books)\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    m1 = doc_term_matrix[0].tolist()\n",
    "    m2 = doc_term_matrix[1].tolist()\n",
    "    n_dice2.append(dice(m1[0],m2[0]))\n",
    "    n_hel2.append(hellinger(m1[0],m2[0]))\n",
    "    n_jac2.append(jaccard(m1[0],m2[0]))\n",
    "    n_jen2.append(distance.jensenshannon(m1[0],m2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "NA282 = {'SpacyCosine':s_cosine2, 'SpacyDice':s_dice2, 'SpacyHellinger':s_hel2, 'SpacyJaccard':s_jac2, 'SpacyJSdivergence':s_jen2, 'NLTKCosine':n_cosine2, 'NLTKDice':n_dice2, 'NLTKHellinger':n_hel2, 'NLTKJaccard':n_jac2, 'NLTKJSdivergence':n_jen2}\n",
    "NA282 = pd.DataFrame(NA282)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "NA28_all = pd.concat([NA28, NA281, NA282], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBL_Version_Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Mt', 'Mk', 'Lk', 'Jn', 'Ac', 'Ro', '1Co', '2Co', 'Ga', 'Eph',\n",
       "       'Php', 'Col', '1Th', '2Th', '1Ti', '2Ti', 'Tit', 'Phm', 'Heb',\n",
       "       'Jas', '1Pe', '2Pe', '1Jn', '2Jn', '3Jn', 'Jud', 'Re'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.BookName.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matthew = df2[df2.BookName == 'Mt']\n",
    "Matthew['Author'] = 'Matthew'\n",
    "Mark = df2[df2.BookName == 'Mk']\n",
    "Mark['Author'] = 'Mark'\n",
    "Luke = df2[df2.BookName == 'Lk']\n",
    "Luke['Author'] = 'Luke'\n",
    "John = df2[df2.BookName == 'Jn']\n",
    "John['Author'] = 'John'\n",
    "Acts = df2[df2.BookName == 'Ac']\n",
    "Acts['Author'] = 'Luke'\n",
    "Luke_Book = pd.concat([Luke,Acts],axis = 0)\n",
    "James = df2[df2.BookName == 'Jas']\n",
    "James['Author'] = 'James'\n",
    "f_Peter = df2[df2.BookName == '1Pe']\n",
    "s_Peter = df2[df2.BookName == '2Pe']\n",
    "Peter_letter = pd.concat([f_Peter, s_Peter], axis = 0)\n",
    "Peter_letter['Author'] = 'Peter'\n",
    "f_John = df2[df2.BookName == '1Jn']\n",
    "s_John = df2[df2.BookName == '2Jn']\n",
    "t_John = df2[df2.BookName == '3Jn']\n",
    "John_letter = pd.concat([f_John, s_John, t_John], axis = 0)\n",
    "John_letter['Author'] = 'John'\n",
    "Jude = df2[df2.BookName == 'Jud']\n",
    "Jude['Author'] = 'Jude'\n",
    "Rev = df2[df2.BookName == 'Re']\n",
    "Rev['Author'] = 'John'\n",
    "John_Book = pd.concat([John,John_letter,Rev],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Romans = df2[df2.BookName == 'Ro']\n",
    "Romans['Author'] = 'Paul'\n",
    "f_cor = df2[df2.BookName == '1Co']\n",
    "f_cor['Author'] = 'Paul'\n",
    "s_cor = df2[df2.BookName == '2Co']\n",
    "s_cor['Author'] = 'Paul'\n",
    "Gal = df2[df2.BookName == 'Ga']\n",
    "Gal['Author'] = 'Paul'\n",
    "Eph = df2[df2.BookName == 'Eph']\n",
    "Eph['Author'] = 'Paul'\n",
    "Phi = df2[df2.BookName == 'Php']\n",
    "Phi['Author'] = 'Paul'\n",
    "Col = df2[df2.BookName == 'Col']\n",
    "Col['Author'] = 'Paul'\n",
    "f_the = df2[df2.BookName == '1Th']\n",
    "f_the['Author'] = 'Paul'\n",
    "s_the = df2[df2.BookName == '2Th']\n",
    "s_the['Author'] = 'Paul'\n",
    "f_tim = df2[df2.BookName == '1Ti']\n",
    "f_tim['Author'] = 'Paul'\n",
    "s_tim = df2[df2.BookName == '2Ti']\n",
    "s_tim['Author'] = 'Paul'\n",
    "Titus = df2[df2.BookName == 'Tit']\n",
    "Titus['Author'] = 'Paul'\n",
    "Philemon = df2[df2.BookName == 'Phm']\n",
    "Philemon['Author'] = 'Paul'\n",
    "Paul_letters = pd.concat([Romans,f_cor,s_cor,Gal,Eph,Phi,Col,f_the,s_the,f_tim,s_tim,Titus,Philemon],axis = 0)\n",
    "Heb = df2[df2.BookName == 'Heb']\n",
    "Heb['Author'] = 'Paul?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = cosine_text(Matthew)\n",
    "g2 = cosine_text(Mark)\n",
    "g3 = cosine_text(Luke)\n",
    "g4 = cosine_text(John)\n",
    "acts = cosine_text(Acts)\n",
    "rms = cosine_text(Romans)\n",
    "fcor = cosine_text(f_cor)\n",
    "scor = cosine_text(s_cor)\n",
    "gal = cosine_text(Gal)\n",
    "eph = cosine_text(Eph)\n",
    "phi = cosine_text(Phi)\n",
    "col = cosine_text(Col)\n",
    "fthe = cosine_text(f_the)\n",
    "sthe = cosine_text(s_the)\n",
    "ftim = cosine_text(f_tim)\n",
    "stim = cosine_text(s_tim)\n",
    "titus = cosine_text(Titus)\n",
    "philemon = cosine_text(Philemon)\n",
    "jams = cosine_text(James)\n",
    "fpet = cosine_text(f_Peter)\n",
    "spet = cosine_text(s_Peter)\n",
    "fjohn = cosine_text(f_John)\n",
    "sjohn = cosine_text(s_John)\n",
    "tjohn = cosine_text(t_John.dropna())\n",
    "jud = cosine_text(Jude)\n",
    "rev = cosine_text(Rev)\n",
    "hbr = cosine_text(Heb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "g11 = word_list(g1)\n",
    "g21 = word_list(g2)\n",
    "g31 = word_list(g3)\n",
    "g41 = word_list(g4)\n",
    "acts1 = word_list(acts)\n",
    "rms1 = word_list(rms)\n",
    "fcor1 = word_list(fcor)\n",
    "scor1 = word_list(scor)\n",
    "gal1 = word_list(gal)\n",
    "eph1 = word_list(eph)\n",
    "phi1 = word_list(phi)\n",
    "col1 = word_list(col)\n",
    "fthe1 = word_list(fthe)\n",
    "sthe1 = word_list(sthe)\n",
    "ftim1 = word_list(ftim)\n",
    "stim1 = word_list(stim)\n",
    "titus1 = word_list(titus)\n",
    "philemon1 = word_list(philemon)\n",
    "jams1 = word_list(jams)\n",
    "fpet1 = word_list(fpet)\n",
    "spet1 = word_list(spet)\n",
    "fjohn1 = word_list(fjohn)\n",
    "sjohn1 = word_list(sjohn)\n",
    "tjohn1 = word_list(tjohn)\n",
    "jud1 = word_list(jud)\n",
    "rev1 = word_list(rev)\n",
    "hbr1 = word_list(hbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [g11,g21,g31,g41,acts1,rms1,fcor1,scor1,gal1,eph1,phi1,col1,fthe1,sthe1,ftim1,stim1,titus1,philemon1,jams1,fpet1,spet1,fjohn1,sjohn1,tjohn1,jud1,rev1]\n",
    "s_dice = []\n",
    "s_hel = []\n",
    "s_jac = []\n",
    "s_jen = []\n",
    "for x in range(0,len(documents)):\n",
    "    \n",
    "    s_dice.append(dice(documents[x][0],documents[x][1]))\n",
    "    s_hel.append(hellinger(documents[x][0],documents[x][1]))\n",
    "    s_jac.append(jaccard(documents[x][0],documents[x][1]))\n",
    "    s_jen.append(distance.jensenshannon(documents[x][0],documents[x][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('el_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_cosine = []\n",
    "documents = [g1,g2,g3,g4,acts,rms,fcor,scor,gal,eph,phi,col,fthe,sthe,ftim,stim,titus,philemon,jams,fpet,spet,fjohn,sjohn,tjohn,jud,rev]\n",
    "bindex = list(range(0,27))\n",
    "doc1 = nlp(hbr)\n",
    "for x in range(0,len(documents)):\n",
    "    doc2 = nlp(documents[x])\n",
    "    similarity = doc1.similarity(doc2)\n",
    "    s_cosine.append(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cosine = []\n",
    "documents = [hbr,g1,g2,g3,g4,acts,rms,fcor,scor,gal,eph,phi,col,fthe,sthe,ftim,stim,titus,philemon,jams,fpet,spet,fjohn,sjohn,tjohn,jud,rev]\n",
    "bindex = list(range(0,27))\n",
    "for x in range(1,len(documents)):\n",
    "    two_books = [hbr,documents[x]]\n",
    "    # Create the Document Term Matrix\n",
    "    count_vectorizer = CountVectorizer(stop_words='greek')\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    sparse_matrix = count_vectorizer.fit_transform(two_books)\n",
    "\n",
    "    # Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    df_m5 = pd.DataFrame(doc_term_matrix, \n",
    "                      columns=count_vectorizer.get_feature_names(), \n",
    "                      index=['Hebrews','Compared Book'])\n",
    "    # df_m5.to_csv(str(bindex[x])+'compared Hebrews term weight matrix.csv', encoding = 'utf-16',sep = '\\t', index=False)\n",
    "    cos = tuple(cosine_similarity(df_m5)[0])[1]\n",
    "    n_cosine.append(cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dice, js divergence, hellinger distance\n",
    "n_dice = []\n",
    "n_hel = []\n",
    "n_jac = []\n",
    "n_jen = []\n",
    "documents = [hbr,g1,g2,g3,g4,acts,rms,fcor,scor,gal,eph,phi,col,fthe,sthe,ftim,stim,titus,philemon,jams,fpet,spet,fjohn,sjohn,tjohn,jud,rev]\n",
    "bindex = list(range(0,len(documents)))\n",
    "for x in range(1,len(documents)):\n",
    "    two_books = [hbr,documents[x]]\n",
    "    count_vectorizer = CountVectorizer(stop_words='greek')\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    sparse_matrix = count_vectorizer.fit_transform(two_books)\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    m1 = doc_term_matrix[0].tolist()\n",
    "    m2 = doc_term_matrix[1].tolist()\n",
    "    n_dice.append(dice(m1[0],m2[0]))\n",
    "    n_hel.append(hellinger(m1[0],m2[0]))\n",
    "    n_jac.append(jaccard(m1[0],m2[0]))\n",
    "    n_jen.append(distance.jensenshannon(m1[0],m2[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "SBL = {'SpacyCosine':s_cosine, 'SpacyDice':s_dice, 'SpacyHellinger':s_hel, 'SpacyJaccard':s_jac, 'SpacyJSdivergence':s_jen, 'NLTKCosine':n_cosine, 'NLTKDice':n_dice, 'NLTKHellinger':n_hel, 'NLTKJaccard':n_jac, 'NLTKJSdivergence':n_jen}\n",
    "SBL = pd.DataFrame(SBL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_s1 = cosine_text(Matthew)\n",
    "mark_s1 = cosine_text(Mark)\n",
    "luke_s1 = cosine_text(Luke_Book)\n",
    "jam_s1 = cosine_text(James)\n",
    "pet_s1 = cosine_text(Peter_letter)\n",
    "john_s1 = cosine_text(John_Book.dropna())\n",
    "jude_s1 = cosine_text(Jude)\n",
    "pl_s1 = cosine_text(Paul_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_s2 = word_list(mat_s1)\n",
    "mark_s2 = word_list(mark_s1)\n",
    "luke_s2 = word_list(luke_s1)\n",
    "jam_s2 = word_list(jam_s1)\n",
    "pet_s2 = word_list(pet_s1)\n",
    "john_s2 = word_list(john_s1)\n",
    "jude_s2 = word_list(jude_s1)\n",
    "pl_s2 = word_list(pl_s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_dice1 = []\n",
    "s_hel1 = []\n",
    "s_jac1 = []\n",
    "s_jen1 = []\n",
    "documents = [pl_s2,mat_s2,mark_s2,luke_s2,jam_s2,pet_s2,john_s2,jude_s2]\n",
    "for x in range(0,len(documents)):\n",
    "    s_dice1.append(dice(documents[x][0],documents[x][1]))\n",
    "    s_hel1.append(hellinger(documents[x][0],documents[x][1]))\n",
    "    s_jac1.append(jaccard(documents[x][0],documents[x][1]))\n",
    "    s_jen1.append(distance.jensenshannon(documents[x][0],documents[x][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_cosine1 = []\n",
    "documents = [pl_s1,mat_s1,mark_s1,luke_s1,jam_s1,pet_s1,john_s1,jude_s1]\n",
    "authorindex = list(range(0,len(documents)))\n",
    "doc1 = nlp(hbr)\n",
    "for x in range(0,len(documents)):\n",
    "    doc2 = nlp(documents[x])\n",
    "    similarity = doc1.similarity(doc2)\n",
    "    s_cosine1.append(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cosine1 = []\n",
    "documents = [pl_s1,mat_s1,mark_s1,luke_s1,jam_s1,pet_s1,john_s1,jude_s1]\n",
    "authorindex = list(range(0,len(documents)))\n",
    "for x in range(0,len(documents)):\n",
    "    two_books = [hbr,documents[x]]\n",
    "    # Create the Document Term Matrix\n",
    "    count_vectorizer = CountVectorizer(stop_words='greek')\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    sparse_matrix = count_vectorizer.fit_transform(two_books)\n",
    "\n",
    "    # Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    df_m6 = pd.DataFrame(doc_term_matrix, \n",
    "                      columns=count_vectorizer.get_feature_names(), \n",
    "                      index=['Paul','compared books'])\n",
    "    # df_m6.to_csv(str(authorindex[x])+'author compare with Paul term weight matrix.csv', encoding = 'utf-8', index=False)\n",
    "    cos = tuple(cosine_similarity(df_m6)[0])[1]\n",
    "    n_cosine1.append(cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dice1 = []\n",
    "n_hel1 = []\n",
    "n_jac1 = []\n",
    "n_jen1 = []\n",
    "documents = [pl_s1,mat_s1,mark_s1,luke_s1,jam_s1,pet_s1,john_s1,jude_s1]\n",
    "authorindex = list(range(0,len(documents)))\n",
    "for x in range(0,len(documents)):\n",
    "    two_books = [hbr,documents[x]]\n",
    "    count_vectorizer = CountVectorizer(stop_words='greek')\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    sparse_matrix = count_vectorizer.fit_transform(two_books)\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    m1 = doc_term_matrix[0].tolist()\n",
    "    m2 = doc_term_matrix[1].tolist()\n",
    "    n_dice1.append(dice(m1[0],m2[0]))\n",
    "    n_hel1.append(hellinger(m1[0],m2[0]))\n",
    "    n_jac1.append(jaccard(m1[0],m2[0]))\n",
    "    n_jen1.append(distance.jensenshannon(m1[0],m2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "SBL1 = {'SpacyCosine':s_cosine1, 'SpacyDice':s_dice1, 'SpacyHellinger':s_hel1, 'SpacyJaccard':s_jac1, 'SpacyJSdivergence':s_jen1, 'NLTKCosine':n_cosine1, 'NLTKDice':n_dice1, 'NLTKHellinger':n_hel1, 'NLTKJaccard':n_jac1, 'NLTKJSdivergence':n_jen1}\n",
    "SBL1 = pd.DataFrame(SBL1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_s4 = word_list1(mat_s1)\n",
    "mark_s4 = word_list1(mark_s1)\n",
    "luke_s4 = word_list1(luke_s1)\n",
    "jam_s4 = word_list1(jam_s1)\n",
    "pet_s4 = word_list1(pet_s1)\n",
    "john_s4 = word_list1(john_s1)\n",
    "jude_s4 = word_list1(jude_s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_dice2 = []\n",
    "s_hel2 = []\n",
    "s_jac2 = []\n",
    "s_jen2 = []\n",
    "documents = [mat_s4,mark_s4,luke_s4,jam_s4,pet_s4,john_s4,jude_s4]\n",
    "for x in range(0,len(documents)):\n",
    "    s_dice2.append(dice(documents[x][0],documents[x][1]))\n",
    "    s_hel2.append(hellinger(documents[x][0],documents[x][1]))\n",
    "    s_jac2.append(jaccard(documents[x][0],documents[x][1]))\n",
    "    s_jen2.append(distance.jensenshannon(documents[x][0],documents[x][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_cosine2 = []\n",
    "documents = [mat_s1,mark_s1,luke_s1,jam_s1,pet_s1,john_s1,jude_s1]\n",
    "authorindex = list(range(0,len(documents)))\n",
    "doc1 = nlp(pl_s1)\n",
    "for x in range(0,len(documents)):\n",
    "    doc2 = nlp(documents[x])\n",
    "    similarity = doc1.similarity(doc2)\n",
    "    s_cosine2.append(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cosine2 = []\n",
    "documents = [mat_s1,mark_s1,luke_s1,jam_s1,pet_s1,john_s1,jude_s1]\n",
    "authorindex = list(range(0,len(documents)))\n",
    "for x in range(0,len(documents)):\n",
    "    two_books = [pl_s1,documents[x]]\n",
    "    # Create the Document Term Matrix\n",
    "    count_vectorizer = CountVectorizer(stop_words='greek')\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    sparse_matrix = count_vectorizer.fit_transform(two_books)\n",
    "\n",
    "    # Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    df_m7 = pd.DataFrame(doc_term_matrix, \n",
    "                      columns=count_vectorizer.get_feature_names(), \n",
    "                      index=['Paul','compared books'])\n",
    "    # df_m6.to_csv(str(authorindex[x])+'author compare with Paul term weight matrix.csv', encoding = 'utf-8', index=False)\n",
    "    cos = tuple(cosine_similarity(df_m7)[0])[1]\n",
    "    n_cosine2.append(cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dice2 = []\n",
    "n_hel2 = []\n",
    "n_jac2 = []\n",
    "n_jen2 = []\n",
    "documents = [mat_s1,mark_s1,luke_s1,jam_s1,pet_s1,john_s1,jude_s1]\n",
    "authorindex = list(range(0,len(documents)))\n",
    "for x in range(0,len(documents)):\n",
    "    two_books = [pl_s1,documents[x]]\n",
    "    count_vectorizer = CountVectorizer(stop_words='greek')\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    sparse_matrix = count_vectorizer.fit_transform(two_books)\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    m1 = doc_term_matrix[0].tolist()\n",
    "    m2 = doc_term_matrix[1].tolist()\n",
    "    n_dice2.append(dice(m1[0],m2[0]))\n",
    "    n_hel2.append(hellinger(m1[0],m2[0]))\n",
    "    n_jac2.append(jaccard(m1[0],m2[0]))\n",
    "    n_jen2.append(distance.jensenshannon(m1[0],m2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "SBL2 = {'SpacyCosine':s_cosine2, 'SpacyDice':s_dice2, 'SpacyHellinger':s_hel2, 'SpacyJaccard':s_jac2, 'SpacyJSdivergence':s_jen2, 'NLTKCosine':n_cosine2, 'NLTKDice':n_dice2, 'NLTKHellinger':n_hel2, 'NLTKJaccard':n_jac2, 'NLTKJSdivergence':n_jen2}\n",
    "SBL2 = pd.DataFrame(SBL2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "SBL_all = pd.concat([SBL, SBL1, SBL2], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tyndale_Version_Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Matthew', 'Mark', 'Luke', 'John', 'Acts', 'Romans',\n",
       "       '1Corinthians', '2Corinthians', 'Galatians', 'Ephesians',\n",
       "       'Philippians', 'Colossians', '1Thessalonians', '2Thessalonians',\n",
       "       '1Timothy', '2Timothy', 'Titus', 'Philemon', 'Hebrews', 'James',\n",
       "       '1Peter', '2Peter', '1John', '2John', '3John', 'Jude',\n",
       "       'Revelation'], dtype=object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.BookName.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matthew = df3[df3.BookName == 'Matthew']\n",
    "Matthew['Author'] = 'Matthew'\n",
    "Mark = df3[df3.BookName == 'Mark']\n",
    "Mark['Author'] = 'Mark'\n",
    "Luke = df3[df3.BookName == 'Luke']\n",
    "Luke['Author'] = 'Luke'\n",
    "John = df3[df3.BookName == 'John']\n",
    "John['Author'] = 'John'\n",
    "Acts = df3[df3.BookName == 'Acts']\n",
    "Acts['Author'] = 'Luke'\n",
    "Luke_Book = pd.concat([Luke,Acts],axis = 0)\n",
    "James = df3[df3.BookName == 'James']\n",
    "James['Author'] = 'James'\n",
    "f_Peter = df3[df3.BookName == '1Peter']\n",
    "s_Peter = df3[df3.BookName == '2Peter']\n",
    "Peter_letter = pd.concat([f_Peter, s_Peter], axis = 0)\n",
    "Peter_letter['Author'] = 'Peter'\n",
    "f_John = df3[df3.BookName == '1John']\n",
    "s_John = df3[df3.BookName == '2John']\n",
    "t_John = df3[df3.BookName == '3John']\n",
    "John_letter = pd.concat([f_John, s_John, t_John], axis = 0)\n",
    "John_letter['Author'] = 'John'\n",
    "Jude = df3[df3.BookName == 'Jude']\n",
    "Jude['Author'] = 'Jude'\n",
    "Rev = df3[df3.BookName == 'Revelation']\n",
    "Rev['Author'] = 'John'\n",
    "John_Book = pd.concat([John,John_letter,Rev],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "Romans = df3[df3.BookName == 'Romans']\n",
    "Romans['Author'] = 'Paul'\n",
    "f_cor = df3[df3.BookName == '1Corinthians']\n",
    "f_cor['Author'] = 'Paul'\n",
    "s_cor = df3[df3.BookName == '2Corinthians']\n",
    "s_cor['Author'] = 'Paul'\n",
    "Gal = df3[df3.BookName == 'Galatians']\n",
    "Gal['Author'] = 'Paul'\n",
    "Eph = df3[df3.BookName == 'Ephesians']\n",
    "Eph['Author'] = 'Paul'\n",
    "Phi = df3[df3.BookName == 'Philippians']\n",
    "Phi['Author'] = 'Paul'\n",
    "Col = df3[df3.BookName == 'Colossians']\n",
    "Col['Author'] = 'Paul'\n",
    "f_the = df3[df3.BookName == '1Thessalonians']\n",
    "f_the['Author'] = 'Paul'\n",
    "s_the = df3[df3.BookName == '2Thessalonians']\n",
    "s_the['Author'] = 'Paul'\n",
    "f_tim = df3[df3.BookName == '1Timothy']\n",
    "f_tim['Author'] = 'Paul'\n",
    "s_tim = df3[df3.BookName == '2Timothy']\n",
    "s_tim['Author'] = 'Paul'\n",
    "Titus = df3[df3.BookName == 'Titus']\n",
    "Titus['Author'] = 'Paul'\n",
    "Philemon = df3[df3.BookName == 'Philemon']\n",
    "Philemon['Author'] = 'Paul'\n",
    "Paul_letters = pd.concat([Romans,f_cor,s_cor,Gal,Eph,Phi,Col,f_the,s_the,f_tim,s_tim,Titus,Philemon],axis = 0)\n",
    "Heb = df3[df3.BookName == 'Hebrews']\n",
    "Heb['Author'] = 'Paul?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = cosine_text(Matthew)\n",
    "g2 = cosine_text(Mark)\n",
    "g3 = cosine_text(Luke)\n",
    "g4 = cosine_text(John)\n",
    "acts = cosine_text(Acts)\n",
    "rms = cosine_text(Romans)\n",
    "fcor = cosine_text(f_cor)\n",
    "scor = cosine_text(s_cor)\n",
    "gal = cosine_text(Gal)\n",
    "eph = cosine_text(Eph)\n",
    "phi = cosine_text(Phi)\n",
    "col = cosine_text(Col)\n",
    "fthe = cosine_text(f_the)\n",
    "sthe = cosine_text(s_the)\n",
    "ftim = cosine_text(f_tim)\n",
    "stim = cosine_text(s_tim)\n",
    "titus = cosine_text(Titus)\n",
    "philemon = cosine_text(Philemon)\n",
    "jams = cosine_text(James)\n",
    "fpet = cosine_text(f_Peter)\n",
    "spet = cosine_text(s_Peter)\n",
    "fjohn = cosine_text(f_John)\n",
    "sjohn = cosine_text(s_John)\n",
    "tjohn = cosine_text(t_John.dropna())\n",
    "jud = cosine_text(Jude)\n",
    "rev = cosine_text(Rev)\n",
    "hbr = cosine_text(Heb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "g11 = word_list(g1)\n",
    "g21 = word_list(g2)\n",
    "g31 = word_list(g3)\n",
    "g41 = word_list(g4)\n",
    "acts1 = word_list(acts)\n",
    "rms1 = word_list(rms)\n",
    "fcor1 = word_list(fcor)\n",
    "scor1 = word_list(scor)\n",
    "gal1 = word_list(gal)\n",
    "eph1 = word_list(eph)\n",
    "phi1 = word_list(phi)\n",
    "col1 = word_list(col)\n",
    "fthe1 = word_list(fthe)\n",
    "sthe1 = word_list(sthe)\n",
    "ftim1 = word_list(ftim)\n",
    "stim1 = word_list(stim)\n",
    "titus1 = word_list(titus)\n",
    "philemon1 = word_list(philemon)\n",
    "jams1 = word_list(jams)\n",
    "fpet1 = word_list(fpet)\n",
    "spet1 = word_list(spet)\n",
    "fjohn1 = word_list(fjohn)\n",
    "sjohn1 = word_list(sjohn)\n",
    "tjohn1 = word_list(tjohn)\n",
    "jud1 = word_list(jud)\n",
    "rev1 = word_list(rev)\n",
    "hbr1 = word_list(hbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [g11,g21,g31,g41,acts1,rms1,fcor1,scor1,gal1,eph1,phi1,col1,fthe1,sthe1,ftim1,stim1,titus1,philemon1,jams1,fpet1,spet1,fjohn1,sjohn1,tjohn1,jud1,rev1]\n",
    "s_dice = []\n",
    "s_hel = []\n",
    "s_jac = []\n",
    "s_jen = []\n",
    "for x in range(0,len(documents)):\n",
    "    \n",
    "    s_dice.append(dice(documents[x][0],documents[x][1]))\n",
    "    s_hel.append(hellinger(documents[x][0],documents[x][1]))\n",
    "    s_jac.append(jaccard(documents[x][0],documents[x][1]))\n",
    "    s_jen.append(distance.jensenshannon(documents[x][0],documents[x][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('el_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_cosine = []\n",
    "documents = [g1,g2,g3,g4,acts,rms,fcor,scor,gal,eph,phi,col,fthe,sthe,ftim,stim,titus,philemon,jams,fpet,spet,fjohn,sjohn,tjohn,jud,rev]\n",
    "bindex = list(range(0,27))\n",
    "doc1 = nlp(hbr)\n",
    "for x in range(0,len(documents)):\n",
    "    doc2 = nlp(documents[x])\n",
    "    similarity = doc1.similarity(doc2)\n",
    "    s_cosine.append(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cosine = []\n",
    "documents = [hbr,g1,g2,g3,g4,acts,rms,fcor,scor,gal,eph,phi,col,fthe,sthe,ftim,stim,titus,philemon,jams,fpet,spet,fjohn,sjohn,tjohn,jud,rev]\n",
    "bindex = list(range(0,27))\n",
    "for x in range(1,len(documents)):\n",
    "    two_books = [hbr,documents[x]]\n",
    "    # Create the Document Term Matrix\n",
    "    count_vectorizer = CountVectorizer(stop_words='greek')\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    sparse_matrix = count_vectorizer.fit_transform(two_books)\n",
    "\n",
    "    # Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    df_m5 = pd.DataFrame(doc_term_matrix, \n",
    "                      columns=count_vectorizer.get_feature_names(), \n",
    "                      index=['Hebrews','Compared Book'])\n",
    "    # df_m5.to_csv(str(bindex[x])+'compared Hebrews term weight matrix.csv', encoding = 'utf-16',sep = '\\t', index=False)\n",
    "    cos = tuple(cosine_similarity(df_m5)[0])[1]\n",
    "    n_cosine.append(cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dice, js divergence, hellinger distance\n",
    "n_dice = []\n",
    "n_hel = []\n",
    "n_jac = []\n",
    "n_jen = []\n",
    "documents = [hbr,g1,g2,g3,g4,acts,rms,fcor,scor,gal,eph,phi,col,fthe,sthe,ftim,stim,titus,philemon,jams,fpet,spet,fjohn,sjohn,tjohn,jud,rev]\n",
    "bindex = list(range(0,len(documents)))\n",
    "for x in range(1,len(documents)):\n",
    "    two_books = [hbr,documents[x]]\n",
    "    count_vectorizer = CountVectorizer(stop_words='greek')\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    sparse_matrix = count_vectorizer.fit_transform(two_books)\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    m1 = doc_term_matrix[0].tolist()\n",
    "    m2 = doc_term_matrix[1].tolist()\n",
    "    n_dice.append(dice(m1[0],m2[0]))\n",
    "    n_hel.append(hellinger(m1[0],m2[0]))\n",
    "    n_jac.append(jaccard(m1[0],m2[0]))\n",
    "    n_jen.append(distance.jensenshannon(m1[0],m2[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tyndale = {'SpacyCosine':s_cosine, 'SpacyDice':s_dice, 'SpacyHellinger':s_hel, 'SpacyJaccard':s_jac, 'SpacyJSdivergence':s_jen, 'NLTKCosine':n_cosine, 'NLTKDice':n_dice, 'NLTKHellinger':n_hel, 'NLTKJaccard':n_jac, 'NLTKJSdivergence':n_jen}\n",
    "Tyndale = pd.DataFrame(Tyndale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_s1 = cosine_text(Matthew)\n",
    "mark_s1 = cosine_text(Mark)\n",
    "luke_s1 = cosine_text(Luke_Book)\n",
    "jam_s1 = cosine_text(James)\n",
    "pet_s1 = cosine_text(Peter_letter)\n",
    "john_s1 = cosine_text(John_Book.dropna())\n",
    "jude_s1 = cosine_text(Jude)\n",
    "pl_s1 = cosine_text(Paul_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_s2 = word_list(mat_s1)\n",
    "mark_s2 = word_list(mark_s1)\n",
    "luke_s2 = word_list(luke_s1)\n",
    "jam_s2 = word_list(jam_s1)\n",
    "pet_s2 = word_list(pet_s1)\n",
    "john_s2 = word_list(john_s1)\n",
    "jude_s2 = word_list(jude_s1)\n",
    "pl_s2 = word_list(pl_s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_dice1 = []\n",
    "s_hel1 = []\n",
    "s_jac1 = []\n",
    "s_jen1 = []\n",
    "documents = [pl_s2,mat_s2,mark_s2,luke_s2,jam_s2,pet_s2,john_s2,jude_s2]\n",
    "for x in range(0,len(documents)):\n",
    "    s_dice1.append(dice(documents[x][0],documents[x][1]))\n",
    "    s_hel1.append(hellinger(documents[x][0],documents[x][1]))\n",
    "    s_jac1.append(jaccard(documents[x][0],documents[x][1]))\n",
    "    s_jen1.append(distance.jensenshannon(documents[x][0],documents[x][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_cosine1 = []\n",
    "documents = [pl_s1,mat_s1,mark_s1,luke_s1,jam_s1,pet_s1,john_s1,jude_s1]\n",
    "authorindex = list(range(0,len(documents)))\n",
    "doc1 = nlp(hbr)\n",
    "for x in range(0,len(documents)):\n",
    "    doc2 = nlp(documents[x])\n",
    "    similarity = doc1.similarity(doc2)\n",
    "    s_cosine1.append(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cosine1 = []\n",
    "documents = [pl_s1,mat_s1,mark_s1,luke_s1,jam_s1,pet_s1,john_s1,jude_s1]\n",
    "authorindex = list(range(0,len(documents)))\n",
    "for x in range(0,len(documents)):\n",
    "    two_books = [hbr,documents[x]]\n",
    "    # Create the Document Term Matrix\n",
    "    count_vectorizer = CountVectorizer(stop_words='greek')\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    sparse_matrix = count_vectorizer.fit_transform(two_books)\n",
    "\n",
    "    # Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    df_m6 = pd.DataFrame(doc_term_matrix, \n",
    "                      columns=count_vectorizer.get_feature_names(), \n",
    "                      index=['Paul','compared books'])\n",
    "    # df_m6.to_csv(str(authorindex[x])+'author compare with Paul term weight matrix.csv', encoding = 'utf-8', index=False)\n",
    "    cos = tuple(cosine_similarity(df_m6)[0])[1]\n",
    "    n_cosine1.append(cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dice1 = []\n",
    "n_hel1 = []\n",
    "n_jac1 = []\n",
    "n_jen1 = []\n",
    "documents = [pl_s1,mat_s1,mark_s1,luke_s1,jam_s1,pet_s1,john_s1,jude_s1]\n",
    "authorindex = list(range(0,len(documents)))\n",
    "for x in range(0,len(documents)):\n",
    "    two_books = [hbr,documents[x]]\n",
    "    count_vectorizer = CountVectorizer(stop_words='greek')\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    sparse_matrix = count_vectorizer.fit_transform(two_books)\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    m1 = doc_term_matrix[0].tolist()\n",
    "    m2 = doc_term_matrix[1].tolist()\n",
    "    n_dice1.append(dice(m1[0],m2[0]))\n",
    "    n_hel1.append(hellinger(m1[0],m2[0]))\n",
    "    n_jac1.append(jaccard(m1[0],m2[0]))\n",
    "    n_jen1.append(distance.jensenshannon(m1[0],m2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tyndale1 = {'SpacyCosine':s_cosine1, 'SpacyDice':s_dice1, 'SpacyHellinger':s_hel1, 'SpacyJaccard':s_jac1, 'SpacyJSdivergence':s_jen1, 'NLTKCosine':n_cosine1, 'NLTKDice':n_dice1, 'NLTKHellinger':n_hel1, 'NLTKJaccard':n_jac1, 'NLTKJSdivergence':n_jen1}\n",
    "Tyndale1 = pd.DataFrame(Tyndale1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_s4 = word_list1(mat_s1)\n",
    "mark_s4 = word_list1(mark_s1)\n",
    "luke_s4 = word_list1(luke_s1)\n",
    "jam_s4 = word_list1(jam_s1)\n",
    "pet_s4 = word_list1(pet_s1)\n",
    "john_s4 = word_list1(john_s1)\n",
    "jude_s4 = word_list1(jude_s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_dice2 = []\n",
    "s_hel2 = []\n",
    "s_jac2 = []\n",
    "s_jen2 = []\n",
    "documents = [mat_s4,mark_s4,luke_s4,jam_s4,pet_s4,john_s4,jude_s4]\n",
    "for x in range(0,len(documents)):\n",
    "    s_dice2.append(dice(documents[x][0],documents[x][1]))\n",
    "    s_hel2.append(hellinger(documents[x][0],documents[x][1]))\n",
    "    s_jac2.append(jaccard(documents[x][0],documents[x][1]))\n",
    "    s_jen2.append(distance.jensenshannon(documents[x][0],documents[x][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_cosine2 = []\n",
    "documents = [mat_s1,mark_s1,luke_s1,jam_s1,pet_s1,john_s1,jude_s1]\n",
    "authorindex = list(range(0,len(documents)))\n",
    "doc1 = nlp(pl_s1)\n",
    "for x in range(0,len(documents)):\n",
    "    doc2 = nlp(documents[x])\n",
    "    similarity = doc1.similarity(doc2)\n",
    "    s_cosine2.append(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cosine2 = []\n",
    "documents = [mat_s1,mark_s1,luke_s1,jam_s1,pet_s1,john_s1,jude_s1]\n",
    "authorindex = list(range(0,len(documents)))\n",
    "for x in range(0,len(documents)):\n",
    "    two_books = [pl_s1,documents[x]]\n",
    "    # Create the Document Term Matrix\n",
    "    count_vectorizer = CountVectorizer(stop_words='greek')\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    sparse_matrix = count_vectorizer.fit_transform(two_books)\n",
    "\n",
    "    # Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    df_m7 = pd.DataFrame(doc_term_matrix, \n",
    "                      columns=count_vectorizer.get_feature_names(), \n",
    "                      index=['Paul','compared books'])\n",
    "    # df_m6.to_csv(str(authorindex[x])+'author compare with Paul term weight matrix.csv', encoding = 'utf-8', index=False)\n",
    "    cos = tuple(cosine_similarity(df_m7)[0])[1]\n",
    "    n_cosine2.append(cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dice2 = []\n",
    "n_hel2 = []\n",
    "n_jac2 = []\n",
    "n_jen2 = []\n",
    "documents = [mat_s1,mark_s1,luke_s1,jam_s1,pet_s1,john_s1,jude_s1]\n",
    "authorindex = list(range(0,len(documents)))\n",
    "for x in range(0,len(documents)):\n",
    "    two_books = [pl_s1,documents[x]]\n",
    "    count_vectorizer = CountVectorizer(stop_words='greek')\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    sparse_matrix = count_vectorizer.fit_transform(two_books)\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    m1 = doc_term_matrix[0].tolist()\n",
    "    m2 = doc_term_matrix[1].tolist()\n",
    "    n_dice2.append(dice(m1[0],m2[0]))\n",
    "    n_hel2.append(hellinger(m1[0],m2[0]))\n",
    "    n_jac2.append(jaccard(m1[0],m2[0]))\n",
    "    n_jen2.append(distance.jensenshannon(m1[0],m2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tyndale2 = {'SpacyCosine':s_cosine2, 'SpacyDice':s_dice2, 'SpacyHellinger':s_hel2, 'SpacyJaccard':s_jac2, 'SpacyJSdivergence':s_jen2, 'NLTKCosine':n_cosine2, 'NLTKDice':n_dice2, 'NLTKHellinger':n_hel2, 'NLTKJaccard':n_jac2, 'NLTKJSdivergence':n_jen2}\n",
    "Tyndale2 = pd.DataFrame(Tyndale2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tyndale_all = pd.concat([Tyndale, Tyndale1, Tyndale2], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_index = pd.concat([NA28_all,SBL_all,Tyndale_all], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_index.to_excel('SpaCy&NLTK_index.xlsx',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
