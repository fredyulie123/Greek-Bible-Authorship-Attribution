{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P7sLFPYnXWfO",
    "outputId": "4a162f82-9830-4189-ff90-275485e9aa60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanza\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/8b/3a9e7a8d8cb14ad6afffc3983b7a7322a3a24d94ebc978a70746fcffc085/stanza-1.1.1-py3-none-any.whl (227kB)\n",
      "\r",
      "\u001b[K     |█▍                              | 10kB 6.1MB/s eta 0:00:01\r",
      "\u001b[K     |██▉                             | 20kB 1.7MB/s eta 0:00:01\r",
      "\u001b[K     |████▎                           | 30kB 2.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████▊                          | 40kB 2.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████▏                        | 51kB 2.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████▋                       | 61kB 2.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 71kB 2.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▌                    | 81kB 2.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 92kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▍                 | 102kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▉                | 112kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▎              | 122kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▊             | 133kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▏           | 143kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 153kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 163kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▌       | 174kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 184kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▍    | 194kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 204kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▎ | 215kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▊| 225kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 235kB 2.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanza) (2.23.0)\n",
      "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from stanza) (1.7.0+cu101)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanza) (1.18.5)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanza) (3.12.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanza) (4.41.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (1.24.3)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.7)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (3.7.4.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (50.3.2)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (1.15.0)\n",
      "Installing collected packages: stanza\n",
      "Successfully installed stanza-1.1.1\n"
     ]
    }
   ],
   "source": [
    "pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hFUC2tJoXkSn"
   },
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nE2283f2XqWT",
    "outputId": "94781918-e40e-4b0e-bf3d-3dc0de337c01"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 10.8MB/s]                    \n",
      "2020-11-10 22:55:19 INFO: Downloading default packages for language: grc (Ancient_Greek)...\n",
      "Downloading http://nlp.stanford.edu/software/stanza/1.1.0/grc/default.zip: 100%|██████████| 131M/131M [00:17<00:00, 7.55MB/s]\n",
      "2020-11-10 22:55:39 INFO: Finished downloading models and saved to /root/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "stanza.download('grc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "q3MttMgHX2CP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import gensim\n",
    "from gensim import corpora,models\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer #tokenizes and counts words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #alternatively, use TfidfTransformer()\n",
    "from sklearn import metrics\n",
    "import requests\n",
    "from sklearn.feature_selection import RFE, chi2\n",
    "from scipy.spatial import distance\n",
    "from pandas.core.frame import DataFrame\n",
    "from nltk import FreqDist\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.matutils import kullback_leibler, jaccard, hellinger, sparse2full\n",
    "import string\n",
    "from scipy.stats import entropy\n",
    "from numpy.linalg import norm\n",
    "from scipy.spatial import distance\n",
    "import spacy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "4yiyNcK3fSHC"
   },
   "outputs": [],
   "source": [
    "from spacy.lang.el import Greek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2bSu9EgabGOi",
    "outputId": "a073daa7-48dd-4841-d1b4-5e243b3f3796"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Getting started.pdf',\n",
       " 'image (1).jpg',\n",
       " 'image.jpg',\n",
       " 'C347414E-901F-4ADD-A460-3B99C03799B4.png',\n",
       " 'Introduction to Scientific Programming and Simulation Using R.pdf',\n",
       " 'RDataScience(1).pdf',\n",
       " 'database group project.gdoc',\n",
       " 'Database Data.gsheet',\n",
       " 'Text Analytics Phase 1.gslides',\n",
       " '2018GMAT课件',\n",
       " 'Untitled presentation (1).gslides',\n",
       " '9526f2636c319b9ed60bdc9c605f7725.mp4',\n",
       " 'Camera Roll.zip',\n",
       " 'WIN_20200323_11_22_38_Pro.mp4',\n",
       " 'Untitled spreadsheet.gsheet',\n",
       " 'Big Data Analytics Proposal.gslides',\n",
       " 'Discover the correlation between Public transportation facilities investment and the frequency of Traffic accident in United States.gdoc',\n",
       " 'Untitled presentation.gslides',\n",
       " 'gmat.exe',\n",
       " 'Web Analytics.zip',\n",
       " 'Data Mining.zip',\n",
       " 'datamodeler-19.2.0.182.1216-x64.zip',\n",
       " 'Bootcamp 课件.zip',\n",
       " 'Torrey (1911).pdf',\n",
       " 'Torrey (1911).gdoc',\n",
       " 'Bootcamp 课件 (1).zip',\n",
       " 'Bible_Data.csv',\n",
       " 'OtherApostlesWork_Data.csv',\n",
       " 'Colab Notebooks',\n",
       " 'drive',\n",
       " 'Peter word_count.jpg',\n",
       " 'John word_count.jpg',\n",
       " 'Jude word_count.jpg',\n",
       " 'Matthew char_count.jpg',\n",
       " 'Mark char_count.jpg',\n",
       " 'Luke char_count.jpg',\n",
       " 'Paul letter char_count.jpg',\n",
       " 'Mark word_count.jpg',\n",
       " 'James char_count.jpg',\n",
       " 'Peter char_count.jpg',\n",
       " 'John char_count.jpg',\n",
       " 'Jude char_count.jpg',\n",
       " 'Luke word_count.jpg',\n",
       " 'Paul letters word_count.jpg',\n",
       " 'Matthew letter ave_length.jpg',\n",
       " 'Mark letter ave_length.jpg',\n",
       " 'Luke ave_length.jpg',\n",
       " 'James word_count.jpg',\n",
       " 'Paul letter ave_length.jpg',\n",
       " 'James ave_length.jpg',\n",
       " 'Peter ave_length.jpg',\n",
       " 'John ave_length.jpg',\n",
       " 'Jude ave_length.jpg',\n",
       " 'John letter word_count.jpg',\n",
       " 'Hebrews word_count.jpg',\n",
       " 'Romans word_count.jpg',\n",
       " 'John letter char_count.jpg',\n",
       " 'Hebrews char_count.jpg',\n",
       " 'Romans char_count.jpg',\n",
       " 'John letters ave_length.jpg',\n",
       " 'Hebrews ave_length.jpg',\n",
       " 'Romans ave_length.jpg',\n",
       " 'Matthew word_count.jpg',\n",
       " 'glove.6B.100d.txt',\n",
       " 'confusion1.jpg',\n",
       " 'Python',\n",
       " 'OtherApostlesWork_Data.xlsx',\n",
       " 'chn_b.csv',\n",
       " 'sparva.csv',\n",
       " 'greektr1550.csv',\n",
       " 'greektr1894.csv',\n",
       " 'greekwhnu.csv',\n",
       " 'authorship',\n",
       " 'NA28_11_09_11PM.csv',\n",
       " 'SBL_11_09_11PM.csv',\n",
       " 'Tyndale_11_09_11PM.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "path = \"/content/drive/My Drive\"\n",
    "\n",
    "os.chdir(path)\n",
    "os.listdir(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ythSPfqwbZId"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('NA28_11_09_11PM.csv', encoding = 'utf-16', sep = '\\t')\n",
    "df2 = pd.read_csv('SBL_11_09_11PM.csv', encoding = 'utf-16', sep = '\\t')\n",
    "df3 = pd.read_csv('Tyndale_11_09_11PM.csv', encoding = 'utf-16', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "AuVgsEO5cC5e"
   },
   "outputs": [],
   "source": [
    "def cosine_text(text):\n",
    "    text_list = []\n",
    "    for t in text['Verse']:\n",
    "        text_list.append(t)\n",
    "\n",
    "    text_str = ' '.join(text_list)\n",
    "    return text_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sQiuuVUhcPC5",
    "outputId": "87dcf968-bf86-4092-df8c-3ef9f39c84f1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 10.3MB/s]                    \n",
      "2020-11-10 23:15:51 INFO: Downloading default packages for language: el (Greek)...\n",
      "Downloading http://nlp.stanford.edu/software/stanza/1.1.0/el/default.zip: 100%|██████████| 224M/224M [01:01<00:00, 3.62MB/s]\n",
      "2020-11-10 23:16:58 INFO: Finished downloading models and saved to /root/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "stanza.download('el')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mw87N_AWcdZ-",
    "outputId": "a2f15ee4-f35a-432c-c7c9-f98ff8265924"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-10 23:17:17 INFO: Loading these models for language: el (Greek):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gdt     |\n",
      "| mwt       | gdt     |\n",
      "| pos       | gdt     |\n",
      "| lemma     | gdt     |\n",
      "=======================\n",
      "\n",
      "2020-11-10 23:17:17 INFO: Use device: cpu\n",
      "2020-11-10 23:17:17 INFO: Loading: tokenize\n",
      "2020-11-10 23:17:17 INFO: Loading: mwt\n",
      "2020-11-10 23:17:17 INFO: Loading: pos\n",
      "2020-11-10 23:17:18 INFO: Loading: lemma\n",
      "2020-11-10 23:17:18 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='el', processors='tokenize,pos,mwt,lemma', tokenize_pretokenized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cLbow5nAcxQ3"
   },
   "outputs": [],
   "source": [
    "def word_list(text):\n",
    "    spacy_stopwords = spacy.lang.el.stop_words.STOP_WORDS\n",
    "    # spacy_stopwords is a hardcoded set\n",
    "    tokens = nlp(text).get('lemma')\n",
    "    tokens1 = nlp(hbr).get('lemma')\n",
    "    #token_list = []\n",
    "    lemma_list = []\n",
    "    lemma_list1 = []\n",
    "    for token in tokens:\n",
    "        if token.lower() not in spacy_stopwords:\n",
    "        #token_list.append(token.text)\n",
    "            lemma_list.append(token)\n",
    "    for token in tokens1:\n",
    "        if token.lower() not in spacy_stopwords:\n",
    "        #token_list.append(token.text)\n",
    "            lemma_list1.append(token)\n",
    "    #token_list\n",
    "    dict1 = {}\n",
    "    dict2 = {}\n",
    "    for key in lemma_list:\n",
    "        dict1[key] = dict1.get(key, 0) + 1\n",
    "    for key in lemma_list1:\n",
    "        dict2[key] = dict2.get(key, 0) + 1\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "    for word in dict1:\n",
    "        if word not in dict2:\n",
    "            l2.append(word)\n",
    "    for word in dict2:\n",
    "        if word not in dict1:\n",
    "            l1.append(word)\n",
    "    for x in l1:\n",
    "        dict1[x] = dict1.get(x,0)\n",
    "    for y in l2:\n",
    "        dict2[y] = dict2.get(y,0)\n",
    "    s_dict1 = sorted(dict1.items())\n",
    "    s_dict2 = sorted(dict2.items())\n",
    "    m1 = []\n",
    "    m2 = []\n",
    "    for m in s_dict1:\n",
    "        m1.append(m[1])\n",
    "    for n in s_dict2:\n",
    "        m2.append(n[1])\n",
    "    return [m1,m2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "SniCuvttdCNv"
   },
   "outputs": [],
   "source": [
    "def dice(im1, im2):\n",
    "    \"\"\"\n",
    "    Computes the Dice coefficient, a measure of set similarity.\n",
    "    Parameters\n",
    "    ----------\n",
    "    im1 : array-like, bool\n",
    "        Any array of arbitrary size. If not boolean, will be converted.\n",
    "    im2 : array-like, bool\n",
    "        Any other array of identical size. If not boolean, will be converted.\n",
    "    Returns\n",
    "    -------\n",
    "    dice : float\n",
    "        Dice coefficient as a float on range [0,1].\n",
    "        Maximum similarity = 1\n",
    "        No similarity = 0\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    The order of inputs for `dice` is irrelevant. The result will be\n",
    "    identical if `im1` and `im2` are switched.\n",
    "    \"\"\"\n",
    "    im1 = np.asarray(im1).astype(np.bool)\n",
    "    im2 = np.asarray(im2).astype(np.bool)\n",
    "\n",
    "    if im1.shape != im2.shape:\n",
    "        raise ValueError(\"Shape mismatch: im1 and im2 must have the same shape.\")\n",
    "\n",
    "    # Compute Dice coefficient\n",
    "    intersection = np.logical_and(im1, im2)\n",
    "\n",
    "    return 2. * intersection.sum() / (im1.sum() + im2.sum()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZnMxZRU8dPw3"
   },
   "outputs": [],
   "source": [
    "def word_list1(text):\n",
    "    spacy_stopwords = spacy.lang.el.stop_words.STOP_WORDS\n",
    "    # spacy_stopwords is a hardcoded set\n",
    "    tokens = nlp(text).get('lemma')\n",
    "    tokens1 = nlp(pl_s1).get('lemma')\n",
    "    #token_list = []\n",
    "    lemma_list = []\n",
    "    lemma_list1 = []\n",
    "    for token in tokens:\n",
    "        if token.lower() not in spacy_stopwords:\n",
    "        #token_list.append(token.text)\n",
    "            lemma_list.append(token)\n",
    "    for token in tokens1:\n",
    "        if token.lower() not in spacy_stopwords:\n",
    "        #token_list.append(token.text)\n",
    "            lemma_list1.append(token)\n",
    "    #token_list\n",
    "    dict1 = {}\n",
    "    dict2 = {}\n",
    "    for key in lemma_list:\n",
    "        dict1[key] = dict1.get(key, 0) + 1\n",
    "    for key in lemma_list1:\n",
    "        dict2[key] = dict2.get(key, 0) + 1\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "    for word in dict1:\n",
    "        if word not in dict2:\n",
    "            l2.append(word)\n",
    "    for word in dict2:\n",
    "        if word not in dict1:\n",
    "            l1.append(word)\n",
    "    for x in l1:\n",
    "        dict1[x] = dict1.get(x,0)\n",
    "    for y in l2:\n",
    "        dict2[y] = dict2.get(y,0)\n",
    "    s_dict1 = sorted(dict1.items())\n",
    "    s_dict2 = sorted(dict2.items())\n",
    "    m1 = []\n",
    "    m2 = []\n",
    "    for m in s_dict1:\n",
    "        m1.append(m[1])\n",
    "    for n in s_dict2:\n",
    "        m2.append(n[1])\n",
    "    return [m1,m2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y24BpU0yeH-Y"
   },
   "source": [
    "## NA28_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "1rSILkzUdYw3"
   },
   "outputs": [],
   "source": [
    "Matthew = df[df.BookName == 'Mt']\n",
    "Matthew['Author'] = 'Matthew'\n",
    "Mark = df[df.BookName == 'Mk']\n",
    "Mark['Author'] = 'Mark'\n",
    "Luke = df[df.BookName == 'Lk']\n",
    "Luke['Author'] = 'Luke'\n",
    "John = df[df.BookName == 'Joh']\n",
    "John['Author'] = 'John'\n",
    "Acts = df[df.BookName == 'Apg']\n",
    "Acts['Author'] = 'Luke'\n",
    "Luke_Book = pd.concat([Luke,Acts],axis = 0)\n",
    "James = df[df.BookName == 'Jak']\n",
    "James['Author'] = 'James'\n",
    "f_Peter = df[df.BookName == '1.Petr']\n",
    "s_Peter = df[df.BookName == '2.Petr']\n",
    "Peter_letter = pd.concat([f_Peter, s_Peter], axis = 0)\n",
    "Peter_letter['Author'] = 'Peter'\n",
    "f_John = df[df.BookName == '1.Joh']\n",
    "s_John = df[df.BookName == '2.Joh']\n",
    "t_John = df[df.BookName == '3.Joh']\n",
    "John_letter = pd.concat([f_John, s_John, t_John], axis = 0)\n",
    "John_letter['Author'] = 'John'\n",
    "Jude = df[df.BookName == 'Jud']\n",
    "Jude['Author'] = 'Jude'\n",
    "Rev = df[df.BookName == 'Offb']\n",
    "Rev['Author'] = 'John'\n",
    "John_Book = pd.concat([John,John_letter,Rev],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "s7kigzB3eUQa"
   },
   "outputs": [],
   "source": [
    "Romans = df[df.BookName == 'Röm']\n",
    "Romans['Author'] = 'Paul'\n",
    "f_cor = df[df.BookName == '1.Kor']\n",
    "f_cor['Author'] = 'Paul'\n",
    "s_cor = df[df.BookName == '2.Kor']\n",
    "s_cor['Author'] = 'Paul'\n",
    "Gal = df[df.BookName == 'Gal']\n",
    "Gal['Author'] = 'Paul'\n",
    "Eph = df[df.BookName == 'Eph']\n",
    "Eph['Author'] = 'Paul'\n",
    "Phi = df[df.BookName == 'Phil']\n",
    "Phi['Author'] = 'Paul'\n",
    "Col = df[df.BookName == 'Kol']\n",
    "Col['Author'] = 'Paul'\n",
    "f_the = df[df.BookName == '1.Thess']\n",
    "f_the['Author'] = 'Paul'\n",
    "s_the = df[df.BookName == '2.Thess']\n",
    "s_the['Author'] = 'Paul'\n",
    "f_tim = df[df.BookName == '1.Tim']\n",
    "f_tim['Author'] = 'Paul'\n",
    "s_tim = df[df.BookName == '2.Tim']\n",
    "s_tim['Author'] = 'Paul'\n",
    "Titus = df[df.BookName == 'Tit']\n",
    "Titus['Author'] = 'Paul'\n",
    "Philemon = df[df.BookName == 'Phlm']\n",
    "Philemon['Author'] = 'Paul'\n",
    "Paul_letters = pd.concat([Romans,f_cor,s_cor,Gal,Eph,Phi,Col,f_the,s_the,f_tim,s_tim,Titus,Philemon],axis = 0)\n",
    "Heb = df[df.BookName == 'heb']\n",
    "Heb['Author'] = 'Paul?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "YcWH-437eaXh"
   },
   "outputs": [],
   "source": [
    "g1 = cosine_text(Matthew)\n",
    "g2 = cosine_text(Mark)\n",
    "g3 = cosine_text(Luke)\n",
    "g4 = cosine_text(John)\n",
    "acts = cosine_text(Acts)\n",
    "rms = cosine_text(Romans)\n",
    "fcor = cosine_text(f_cor)\n",
    "scor = cosine_text(s_cor)\n",
    "gal = cosine_text(Gal)\n",
    "eph = cosine_text(Eph)\n",
    "phi = cosine_text(Phi)\n",
    "col = cosine_text(Col)\n",
    "fthe = cosine_text(f_the)\n",
    "sthe = cosine_text(s_the)\n",
    "ftim = cosine_text(f_tim)\n",
    "stim = cosine_text(s_tim)\n",
    "titus = cosine_text(Titus)\n",
    "philemon = cosine_text(Philemon)\n",
    "jams = cosine_text(James)\n",
    "fpet = cosine_text(f_Peter)\n",
    "spet = cosine_text(s_Peter)\n",
    "fjohn = cosine_text(f_John)\n",
    "sjohn = cosine_text(s_John)\n",
    "tjohn = cosine_text(t_John.dropna())\n",
    "jud = cosine_text(Jude)\n",
    "rev = cosine_text(Rev)\n",
    "hbr = cosine_text(Heb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "QAhkZ75qehGV"
   },
   "outputs": [],
   "source": [
    "g11 = word_list(g1)\n",
    "g21 = word_list(g2)\n",
    "g31 = word_list(g3)\n",
    "g41 = word_list(g4)\n",
    "acts1 = word_list(acts)\n",
    "rms1 = word_list(rms)\n",
    "fcor1 = word_list(fcor)\n",
    "scor1 = word_list(scor)\n",
    "gal1 = word_list(gal)\n",
    "eph1 = word_list(eph)\n",
    "phi1 = word_list(phi)\n",
    "col1 = word_list(col)\n",
    "fthe1 = word_list(fthe)\n",
    "sthe1 = word_list(sthe)\n",
    "ftim1 = word_list(ftim)\n",
    "stim1 = word_list(stim)\n",
    "titus1 = word_list(titus)\n",
    "philemon1 = word_list(philemon)\n",
    "jams1 = word_list(jams)\n",
    "fpet1 = word_list(fpet)\n",
    "spet1 = word_list(spet)\n",
    "fjohn1 = word_list(fjohn)\n",
    "sjohn1 = word_list(sjohn)\n",
    "tjohn1 = word_list(tjohn)\n",
    "jud1 = word_list(jud)\n",
    "rev1 = word_list(rev)\n",
    "hbr1 = word_list(hbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "1D5nCS8KgJUd"
   },
   "outputs": [],
   "source": [
    "documents = [g11,g21,g31,g41,acts1,rms1,fcor1,scor1,gal1,eph1,phi1,col1,fthe1,sthe1,ftim1,stim1,titus1,philemon1,jams1,fpet1,spet1,fjohn1,sjohn1,tjohn1,jud1,rev1]\n",
    "s_dice = []\n",
    "s_hel = []\n",
    "s_jac = []\n",
    "s_jen = []\n",
    "s_cosine = []\n",
    "for x in range(0,len(documents)):\n",
    "    \n",
    "    s_dice.append(dice(documents[x][0],documents[x][1]))\n",
    "    s_hel.append(hellinger(documents[x][0],documents[x][1]))\n",
    "    s_jac.append(jaccard(documents[x][0],documents[x][1]))\n",
    "    s_jen.append(distance.jensenshannon(documents[x][0],documents[x][1]))\n",
    "    cos = tuple(cosine_similarity(documents[x])[0])\n",
    "    s_cosine.append(cos[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "jCntdj9ChBrG"
   },
   "outputs": [],
   "source": [
    "NA28 = {'StanzaCosine':s_cosine, 'StanzaDice':s_dice, 'StanzaHellinger':s_hel, 'StanzaJaccard':s_jac, 'StanzaJSdivergence':s_jen}\n",
    "NA28 = pd.DataFrame(NA28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "7DF9PDCshdUF"
   },
   "outputs": [],
   "source": [
    "mat_s1 = cosine_text(Matthew)\n",
    "mark_s1 = cosine_text(Mark)\n",
    "luke_s1 = cosine_text(Luke_Book)\n",
    "jam_s1 = cosine_text(James)\n",
    "pet_s1 = cosine_text(Peter_letter)\n",
    "john_s1 = cosine_text(John_Book.dropna())\n",
    "jude_s1 = cosine_text(Jude)\n",
    "pl_s1 = cosine_text(Paul_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "QI1kXgwaiXI3"
   },
   "outputs": [],
   "source": [
    "mat_s2 = word_list(mat_s1)\n",
    "mark_s2 = word_list(mark_s1)\n",
    "luke_s2 = word_list(luke_s1)\n",
    "jam_s2 = word_list(jam_s1)\n",
    "pet_s2 = word_list(pet_s1)\n",
    "john_s2 = word_list(john_s1)\n",
    "jude_s2 = word_list(jude_s1)\n",
    "pl_s2 = word_list(pl_s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "hg9UTcUVicow"
   },
   "outputs": [],
   "source": [
    "s_dice1 = []\n",
    "s_hel1 = []\n",
    "s_jac1 = []\n",
    "s_jen1 = []\n",
    "s_cosine1 = []\n",
    "documents = [pl_s2,mat_s2,mark_s2,luke_s2,jam_s2,pet_s2,john_s2,jude_s2]\n",
    "for x in range(0,len(documents)):\n",
    "    s_dice1.append(dice(documents[x][0],documents[x][1]))\n",
    "    s_hel1.append(hellinger(documents[x][0],documents[x][1]))\n",
    "    s_jac1.append(jaccard(documents[x][0],documents[x][1]))\n",
    "    s_jen1.append(distance.jensenshannon(documents[x][0],documents[x][1]))\n",
    "    cos = tuple(cosine_similarity(documents[x])[0])\n",
    "    s_cosine1.append(cos[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "4weyaQgBjjx0"
   },
   "outputs": [],
   "source": [
    "NA281 = {'StanzaCosine':s_cosine1, 'StanzaDice':s_dice1, 'StanzaHellinger':s_hel1, 'StanzaJaccard':s_jac1, 'StanzaJSdivergence':s_jen1}\n",
    "NA281 = pd.DataFrame(NA281)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "QDZqcgm1kHXs"
   },
   "outputs": [],
   "source": [
    "mat_s4 = word_list1(mat_s1)\n",
    "mark_s4 = word_list1(mark_s1)\n",
    "luke_s4 = word_list1(luke_s1)\n",
    "jam_s4 = word_list1(jam_s1)\n",
    "pet_s4 = word_list1(pet_s1)\n",
    "john_s4 = word_list1(john_s1)\n",
    "jude_s4 = word_list1(jude_s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Ur4SN-uykQkf"
   },
   "outputs": [],
   "source": [
    "s_dice2 = []\n",
    "s_hel2 = []\n",
    "s_jac2 = []\n",
    "s_jen2 = []\n",
    "s_cosine2 = []\n",
    "documents = [mat_s4,mark_s4,luke_s4,jam_s4,pet_s4,john_s4,jude_s4]\n",
    "for x in range(0,len(documents)):\n",
    "    s_dice2.append(dice(documents[x][0],documents[x][1]))\n",
    "    s_hel2.append(hellinger(documents[x][0],documents[x][1]))\n",
    "    s_jac2.append(jaccard(documents[x][0],documents[x][1]))\n",
    "    s_jen2.append(distance.jensenshannon(documents[x][0],documents[x][1]))\n",
    "    cos = tuple(cosine_similarity(documents[x])[0])\n",
    "    s_cosine2.append(cos[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "7kyI55ZwksVO"
   },
   "outputs": [],
   "source": [
    "NA282 = {'StanzaCosine':s_cosine2, 'StanzaDice':s_dice2, 'StanzaHellinger':s_hel2, 'StanzaJaccard':s_jac2, 'StanzaJSdivergence':s_jen2}\n",
    "NA282 = pd.DataFrame(NA282)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "OGD3I3KUmVkv"
   },
   "outputs": [],
   "source": [
    "NA28_all = pd.concat([NA28, NA281, NA282], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnBGScSEmeFO"
   },
   "source": [
    "## SBL_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "GlHJe-ABmdaK"
   },
   "outputs": [],
   "source": [
    "Matthew = df2[df2.BookName == 'Mt']\n",
    "Matthew['Author'] = 'Matthew'\n",
    "Mark = df2[df2.BookName == 'Mk']\n",
    "Mark['Author'] = 'Mark'\n",
    "Luke = df2[df2.BookName == 'Lk']\n",
    "Luke['Author'] = 'Luke'\n",
    "John = df2[df2.BookName == 'Jn']\n",
    "John['Author'] = 'John'\n",
    "Acts = df2[df2.BookName == 'Ac']\n",
    "Acts['Author'] = 'Luke'\n",
    "Luke_Book = pd.concat([Luke,Acts],axis = 0)\n",
    "James = df2[df2.BookName == 'Jas']\n",
    "James['Author'] = 'James'\n",
    "f_Peter = df2[df2.BookName == '1Pe']\n",
    "s_Peter = df2[df2.BookName == '2Pe']\n",
    "Peter_letter = pd.concat([f_Peter, s_Peter], axis = 0)\n",
    "Peter_letter['Author'] = 'Peter'\n",
    "f_John = df2[df2.BookName == '1Jn']\n",
    "s_John = df2[df2.BookName == '2Jn']\n",
    "t_John = df2[df2.BookName == '3Jn']\n",
    "John_letter = pd.concat([f_John, s_John, t_John], axis = 0)\n",
    "John_letter['Author'] = 'John'\n",
    "Jude = df2[df2.BookName == 'Jud']\n",
    "Jude['Author'] = 'Jude'\n",
    "Rev = df2[df2.BookName == 'Re']\n",
    "Rev['Author'] = 'John'\n",
    "John_Book = pd.concat([John,John_letter,Rev],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "VfaEEFU-mwKE"
   },
   "outputs": [],
   "source": [
    "Romans = df2[df2.BookName == 'Ro']\n",
    "Romans['Author'] = 'Paul'\n",
    "f_cor = df2[df2.BookName == '1Co']\n",
    "f_cor['Author'] = 'Paul'\n",
    "s_cor = df2[df2.BookName == '2Co']\n",
    "s_cor['Author'] = 'Paul'\n",
    "Gal = df2[df2.BookName == 'Ga']\n",
    "Gal['Author'] = 'Paul'\n",
    "Eph = df2[df2.BookName == 'Eph']\n",
    "Eph['Author'] = 'Paul'\n",
    "Phi = df2[df2.BookName == 'Php']\n",
    "Phi['Author'] = 'Paul'\n",
    "Col = df2[df2.BookName == 'Col']\n",
    "Col['Author'] = 'Paul'\n",
    "f_the = df2[df2.BookName == '1Th']\n",
    "f_the['Author'] = 'Paul'\n",
    "s_the = df2[df2.BookName == '2Th']\n",
    "s_the['Author'] = 'Paul'\n",
    "f_tim = df2[df2.BookName == '1Ti']\n",
    "f_tim['Author'] = 'Paul'\n",
    "s_tim = df2[df2.BookName == '2Ti']\n",
    "s_tim['Author'] = 'Paul'\n",
    "Titus = df2[df2.BookName == 'Tit']\n",
    "Titus['Author'] = 'Paul'\n",
    "Philemon = df2[df2.BookName == 'Phm']\n",
    "Philemon['Author'] = 'Paul'\n",
    "Paul_letters = pd.concat([Romans,f_cor,s_cor,Gal,Eph,Phi,Col,f_the,s_the,f_tim,s_tim,Titus,Philemon],axis = 0)\n",
    "Heb = df2[df2.BookName == 'Heb']\n",
    "Heb['Author'] = 'Paul?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "TKYiNSOjm0gN"
   },
   "outputs": [],
   "source": [
    "g1 = cosine_text(Matthew)\n",
    "g2 = cosine_text(Mark)\n",
    "g3 = cosine_text(Luke)\n",
    "g4 = cosine_text(John)\n",
    "acts = cosine_text(Acts)\n",
    "rms = cosine_text(Romans)\n",
    "fcor = cosine_text(f_cor)\n",
    "scor = cosine_text(s_cor)\n",
    "gal = cosine_text(Gal)\n",
    "eph = cosine_text(Eph)\n",
    "phi = cosine_text(Phi)\n",
    "col = cosine_text(Col)\n",
    "fthe = cosine_text(f_the)\n",
    "sthe = cosine_text(s_the)\n",
    "ftim = cosine_text(f_tim)\n",
    "stim = cosine_text(s_tim)\n",
    "titus = cosine_text(Titus)\n",
    "philemon = cosine_text(Philemon)\n",
    "jams = cosine_text(James)\n",
    "fpet = cosine_text(f_Peter)\n",
    "spet = cosine_text(s_Peter)\n",
    "fjohn = cosine_text(f_John)\n",
    "sjohn = cosine_text(s_John)\n",
    "tjohn = cosine_text(t_John.dropna())\n",
    "jud = cosine_text(Jude)\n",
    "rev = cosine_text(Rev)\n",
    "hbr = cosine_text(Heb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "iPF677Ycm6m-"
   },
   "outputs": [],
   "source": [
    "g11 = word_list(g1)\n",
    "g21 = word_list(g2)\n",
    "g31 = word_list(g3)\n",
    "g41 = word_list(g4)\n",
    "acts1 = word_list(acts)\n",
    "rms1 = word_list(rms)\n",
    "fcor1 = word_list(fcor)\n",
    "scor1 = word_list(scor)\n",
    "gal1 = word_list(gal)\n",
    "eph1 = word_list(eph)\n",
    "phi1 = word_list(phi)\n",
    "col1 = word_list(col)\n",
    "fthe1 = word_list(fthe)\n",
    "sthe1 = word_list(sthe)\n",
    "ftim1 = word_list(ftim)\n",
    "stim1 = word_list(stim)\n",
    "titus1 = word_list(titus)\n",
    "philemon1 = word_list(philemon)\n",
    "jams1 = word_list(jams)\n",
    "fpet1 = word_list(fpet)\n",
    "spet1 = word_list(spet)\n",
    "fjohn1 = word_list(fjohn)\n",
    "sjohn1 = word_list(sjohn)\n",
    "tjohn1 = word_list(tjohn)\n",
    "jud1 = word_list(jud)\n",
    "rev1 = word_list(rev)\n",
    "hbr1 = word_list(hbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "DbMtxpInnKWT"
   },
   "outputs": [],
   "source": [
    "documents = [g11,g21,g31,g41,acts1,rms1,fcor1,scor1,gal1,eph1,phi1,col1,fthe1,sthe1,ftim1,stim1,titus1,philemon1,jams1,fpet1,spet1,fjohn1,sjohn1,tjohn1,jud1,rev1]\n",
    "s_dice = []\n",
    "s_hel = []\n",
    "s_jac = []\n",
    "s_jen = []\n",
    "s_cosine = []\n",
    "for x in range(0,len(documents)):\n",
    "    \n",
    "    s_dice.append(dice(documents[x][0],documents[x][1]))\n",
    "    s_hel.append(hellinger(documents[x][0],documents[x][1]))\n",
    "    s_jac.append(jaccard(documents[x][0],documents[x][1]))\n",
    "    s_jen.append(distance.jensenshannon(documents[x][0],documents[x][1]))\n",
    "    cos = tuple(cosine_similarity(documents[x])[0])\n",
    "    s_cosine.append(cos[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "ivvTfFy-pY9e"
   },
   "outputs": [],
   "source": [
    "SBL = {'StanzaCosine':s_cosine, 'StanzaDice':s_dice, 'StanzaHellinger':s_hel, 'StanzaJaccard':s_jac, 'StanzaJSdivergence':s_jen}\n",
    "SBL = pd.DataFrame(SBL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "o3mvRnjppkU0"
   },
   "outputs": [],
   "source": [
    "mat_s1 = cosine_text(Matthew)\n",
    "mark_s1 = cosine_text(Mark)\n",
    "luke_s1 = cosine_text(Luke_Book)\n",
    "jam_s1 = cosine_text(James)\n",
    "pet_s1 = cosine_text(Peter_letter)\n",
    "john_s1 = cosine_text(John_Book.dropna())\n",
    "jude_s1 = cosine_text(Jude)\n",
    "pl_s1 = cosine_text(Paul_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "sGxytETQprI0"
   },
   "outputs": [],
   "source": [
    "mat_s2 = word_list(mat_s1)\n",
    "mark_s2 = word_list(mark_s1)\n",
    "luke_s2 = word_list(luke_s1)\n",
    "jam_s2 = word_list(jam_s1)\n",
    "pet_s2 = word_list(pet_s1)\n",
    "john_s2 = word_list(john_s1)\n",
    "jude_s2 = word_list(jude_s1)\n",
    "pl_s2 = word_list(pl_s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "62O4QDJkpTKp"
   },
   "outputs": [],
   "source": [
    "s_dice1 = []\n",
    "s_hel1 = []\n",
    "s_jac1 = []\n",
    "s_jen1 = []\n",
    "s_cosine1 = []\n",
    "documents = [pl_s2,mat_s2,mark_s2,luke_s2,jam_s2,pet_s2,john_s2,jude_s2]\n",
    "for x in range(0,len(documents)):\n",
    "    s_dice1.append(dice(documents[x][0],documents[x][1]))\n",
    "    s_hel1.append(hellinger(documents[x][0],documents[x][1]))\n",
    "    s_jac1.append(jaccard(documents[x][0],documents[x][1]))\n",
    "    s_jen1.append(distance.jensenshannon(documents[x][0],documents[x][1]))\n",
    "    cos = tuple(cosine_similarity(documents[x])[0])\n",
    "    s_cosine1.append(cos[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "N5csbVPIpzx-"
   },
   "outputs": [],
   "source": [
    "SBL1 = {'StanzaCosine':s_cosine1, 'StanzaDice':s_dice1, 'StanzaHellinger':s_hel1, 'StanzaJaccard':s_jac1, 'StanzaJSdivergence':s_jen1}\n",
    "SBL1 = pd.DataFrame(SBL1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "um5geDMsqAzZ"
   },
   "outputs": [],
   "source": [
    "mat_s4 = word_list1(mat_s1)\n",
    "mark_s4 = word_list1(mark_s1)\n",
    "luke_s4 = word_list1(luke_s1)\n",
    "jam_s4 = word_list1(jam_s1)\n",
    "pet_s4 = word_list1(pet_s1)\n",
    "john_s4 = word_list1(john_s1)\n",
    "jude_s4 = word_list1(jude_s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "GUvgHEQYqFTu"
   },
   "outputs": [],
   "source": [
    "s_dice2 = []\n",
    "s_hel2 = []\n",
    "s_jac2 = []\n",
    "s_jen2 = []\n",
    "s_cosine2 = []\n",
    "documents = [mat_s4,mark_s4,luke_s4,jam_s4,pet_s4,john_s4,jude_s4]\n",
    "for x in range(0,len(documents)):\n",
    "    s_dice2.append(dice(documents[x][0],documents[x][1]))\n",
    "    s_hel2.append(hellinger(documents[x][0],documents[x][1]))\n",
    "    s_jac2.append(jaccard(documents[x][0],documents[x][1]))\n",
    "    s_jen2.append(distance.jensenshannon(documents[x][0],documents[x][1]))\n",
    "    cos = tuple(cosine_similarity(documents[x])[0])\n",
    "    s_cosine2.append(cos[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "rJ32W-jIqI1e"
   },
   "outputs": [],
   "source": [
    "SBL2 = {'StanzaCosine':s_cosine2, 'StanzaDice':s_dice2, 'StanzaHellinger':s_hel2, 'StanzaJaccard':s_jac2, 'StanzaJSdivergence':s_jen2}\n",
    "SBL2 = pd.DataFrame(SBL2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "kGHz4XhqqQL7"
   },
   "outputs": [],
   "source": [
    "SBL_all = pd.concat([SBL, SBL1, SBL2], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PW2ZKLWwqbGO"
   },
   "source": [
    "## Tyndale_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "8z8e2t7RqgcJ"
   },
   "outputs": [],
   "source": [
    "Matthew = df3[df3.BookName == 'Matthew']\n",
    "Matthew['Author'] = 'Matthew'\n",
    "Mark = df3[df3.BookName == 'Mark']\n",
    "Mark['Author'] = 'Mark'\n",
    "Luke = df3[df3.BookName == 'Luke']\n",
    "Luke['Author'] = 'Luke'\n",
    "John = df3[df3.BookName == 'John']\n",
    "John['Author'] = 'John'\n",
    "Acts = df3[df3.BookName == 'Acts']\n",
    "Acts['Author'] = 'Luke'\n",
    "Luke_Book = pd.concat([Luke,Acts],axis = 0)\n",
    "James = df3[df3.BookName == 'James']\n",
    "James['Author'] = 'James'\n",
    "f_Peter = df3[df3.BookName == '1Peter']\n",
    "s_Peter = df3[df3.BookName == '2Peter']\n",
    "Peter_letter = pd.concat([f_Peter, s_Peter], axis = 0)\n",
    "Peter_letter['Author'] = 'Peter'\n",
    "f_John = df3[df3.BookName == '1John']\n",
    "s_John = df3[df3.BookName == '2John']\n",
    "t_John = df3[df3.BookName == '3John']\n",
    "John_letter = pd.concat([f_John, s_John, t_John], axis = 0)\n",
    "John_letter['Author'] = 'John'\n",
    "Jude = df3[df3.BookName == 'Jude']\n",
    "Jude['Author'] = 'Jude'\n",
    "Rev = df3[df3.BookName == 'Revelation']\n",
    "Rev['Author'] = 'John'\n",
    "John_Book = pd.concat([John,John_letter,Rev],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "RDNYNl1vqpBh"
   },
   "outputs": [],
   "source": [
    "Romans = df3[df3.BookName == 'Romans']\n",
    "Romans['Author'] = 'Paul'\n",
    "f_cor = df3[df3.BookName == '1Corinthians']\n",
    "f_cor['Author'] = 'Paul'\n",
    "s_cor = df3[df3.BookName == '2Corinthians']\n",
    "s_cor['Author'] = 'Paul'\n",
    "Gal = df3[df3.BookName == 'Galatians']\n",
    "Gal['Author'] = 'Paul'\n",
    "Eph = df3[df3.BookName == 'Ephesians']\n",
    "Eph['Author'] = 'Paul'\n",
    "Phi = df3[df3.BookName == 'Philippians']\n",
    "Phi['Author'] = 'Paul'\n",
    "Col = df3[df3.BookName == 'Colossians']\n",
    "Col['Author'] = 'Paul'\n",
    "f_the = df3[df3.BookName == '1Thessalonians']\n",
    "f_the['Author'] = 'Paul'\n",
    "s_the = df3[df3.BookName == '2Thessalonians']\n",
    "s_the['Author'] = 'Paul'\n",
    "f_tim = df3[df3.BookName == '1Timothy']\n",
    "f_tim['Author'] = 'Paul'\n",
    "s_tim = df3[df3.BookName == '2Timothy']\n",
    "s_tim['Author'] = 'Paul'\n",
    "Titus = df3[df3.BookName == 'Titus']\n",
    "Titus['Author'] = 'Paul'\n",
    "Philemon = df3[df3.BookName == 'Philemon']\n",
    "Philemon['Author'] = 'Paul'\n",
    "Paul_letters = pd.concat([Romans,f_cor,s_cor,Gal,Eph,Phi,Col,f_the,s_the,f_tim,s_tim,Titus,Philemon],axis = 0)\n",
    "Heb = df3[df3.BookName == 'Hebrews']\n",
    "Heb['Author'] = 'Paul?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "NlCxhYSFquCf"
   },
   "outputs": [],
   "source": [
    "g1 = cosine_text(Matthew)\n",
    "g2 = cosine_text(Mark)\n",
    "g3 = cosine_text(Luke)\n",
    "g4 = cosine_text(John)\n",
    "acts = cosine_text(Acts)\n",
    "rms = cosine_text(Romans)\n",
    "fcor = cosine_text(f_cor)\n",
    "scor = cosine_text(s_cor)\n",
    "gal = cosine_text(Gal)\n",
    "eph = cosine_text(Eph)\n",
    "phi = cosine_text(Phi)\n",
    "col = cosine_text(Col)\n",
    "fthe = cosine_text(f_the)\n",
    "sthe = cosine_text(s_the)\n",
    "ftim = cosine_text(f_tim)\n",
    "stim = cosine_text(s_tim)\n",
    "titus = cosine_text(Titus)\n",
    "philemon = cosine_text(Philemon)\n",
    "jams = cosine_text(James)\n",
    "fpet = cosine_text(f_Peter)\n",
    "spet = cosine_text(s_Peter)\n",
    "fjohn = cosine_text(f_John)\n",
    "sjohn = cosine_text(s_John)\n",
    "tjohn = cosine_text(t_John.dropna())\n",
    "jud = cosine_text(Jude)\n",
    "rev = cosine_text(Rev)\n",
    "hbr = cosine_text(Heb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "SRLT648tq0cu"
   },
   "outputs": [],
   "source": [
    "g11 = word_list(g1)\n",
    "g21 = word_list(g2)\n",
    "g31 = word_list(g3)\n",
    "g41 = word_list(g4)\n",
    "acts1 = word_list(acts)\n",
    "rms1 = word_list(rms)\n",
    "fcor1 = word_list(fcor)\n",
    "scor1 = word_list(scor)\n",
    "gal1 = word_list(gal)\n",
    "eph1 = word_list(eph)\n",
    "phi1 = word_list(phi)\n",
    "col1 = word_list(col)\n",
    "fthe1 = word_list(fthe)\n",
    "sthe1 = word_list(sthe)\n",
    "ftim1 = word_list(ftim)\n",
    "stim1 = word_list(stim)\n",
    "titus1 = word_list(titus)\n",
    "philemon1 = word_list(philemon)\n",
    "jams1 = word_list(jams)\n",
    "fpet1 = word_list(fpet)\n",
    "spet1 = word_list(spet)\n",
    "fjohn1 = word_list(fjohn)\n",
    "sjohn1 = word_list(sjohn)\n",
    "tjohn1 = word_list(tjohn)\n",
    "jud1 = word_list(jud)\n",
    "rev1 = word_list(rev)\n",
    "hbr1 = word_list(hbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "PFkQ2RlRrMJu"
   },
   "outputs": [],
   "source": [
    "documents = [g11,g21,g31,g41,acts1,rms1,fcor1,scor1,gal1,eph1,phi1,col1,fthe1,sthe1,ftim1,stim1,titus1,philemon1,jams1,fpet1,spet1,fjohn1,sjohn1,tjohn1,jud1,rev1]\n",
    "s_dice = []\n",
    "s_hel = []\n",
    "s_jac = []\n",
    "s_jen = []\n",
    "s_cosine = []\n",
    "for x in range(0,len(documents)):\n",
    "    \n",
    "    s_dice.append(dice(documents[x][0],documents[x][1]))\n",
    "    s_hel.append(hellinger(documents[x][0],documents[x][1]))\n",
    "    s_jac.append(jaccard(documents[x][0],documents[x][1]))\n",
    "    s_jen.append(distance.jensenshannon(documents[x][0],documents[x][1]))\n",
    "    cos = tuple(cosine_similarity(documents[x])[0])\n",
    "    s_cosine.append(cos[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "Sk5908mMrTam"
   },
   "outputs": [],
   "source": [
    "Tyndale = {'StanzaCosine':s_cosine, 'StanzaDice':s_dice, 'StanzaHellinger':s_hel, 'StanzaJaccard':s_jac, 'StanzaJSdivergence':s_jen}\n",
    "Tyndale = pd.DataFrame(Tyndale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "uWeu22qkrchx"
   },
   "outputs": [],
   "source": [
    "mat_s1 = cosine_text(Matthew)\n",
    "mark_s1 = cosine_text(Mark)\n",
    "luke_s1 = cosine_text(Luke_Book)\n",
    "jam_s1 = cosine_text(James)\n",
    "pet_s1 = cosine_text(Peter_letter)\n",
    "john_s1 = cosine_text(John_Book.dropna())\n",
    "jude_s1 = cosine_text(Jude)\n",
    "pl_s1 = cosine_text(Paul_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "a18agRtOrgow"
   },
   "outputs": [],
   "source": [
    "mat_s2 = word_list(mat_s1)\n",
    "mark_s2 = word_list(mark_s1)\n",
    "luke_s2 = word_list(luke_s1)\n",
    "jam_s2 = word_list(jam_s1)\n",
    "pet_s2 = word_list(pet_s1)\n",
    "john_s2 = word_list(john_s1)\n",
    "jude_s2 = word_list(jude_s1)\n",
    "pl_s2 = word_list(pl_s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "vO88g5DQrlhA"
   },
   "outputs": [],
   "source": [
    "s_dice1 = []\n",
    "s_hel1 = []\n",
    "s_jac1 = []\n",
    "s_jen1 = []\n",
    "s_cosine1 = []\n",
    "documents = [pl_s2,mat_s2,mark_s2,luke_s2,jam_s2,pet_s2,john_s2,jude_s2]\n",
    "for x in range(0,len(documents)):\n",
    "    s_dice1.append(dice(documents[x][0],documents[x][1]))\n",
    "    s_hel1.append(hellinger(documents[x][0],documents[x][1]))\n",
    "    s_jac1.append(jaccard(documents[x][0],documents[x][1]))\n",
    "    s_jen1.append(distance.jensenshannon(documents[x][0],documents[x][1]))\n",
    "    cos = tuple(cosine_similarity(documents[x])[0])\n",
    "    s_cosine1.append(cos[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "E6OPfE7NrrEW"
   },
   "outputs": [],
   "source": [
    "Tyndale1 = {'StanzaCosine':s_cosine1, 'StanzaDice':s_dice1, 'StanzaHellinger':s_hel1, 'StanzaJaccard':s_jac1, 'StanzaJSdivergence':s_jen1}\n",
    "Tyndale1 = pd.DataFrame(Tyndale1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "u2G4vwhmr1h5"
   },
   "outputs": [],
   "source": [
    "mat_s4 = word_list1(mat_s1)\n",
    "mark_s4 = word_list1(mark_s1)\n",
    "luke_s4 = word_list1(luke_s1)\n",
    "jam_s4 = word_list1(jam_s1)\n",
    "pet_s4 = word_list1(pet_s1)\n",
    "john_s4 = word_list1(john_s1)\n",
    "jude_s4 = word_list1(jude_s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "zg_zCQ7Ur5qt"
   },
   "outputs": [],
   "source": [
    "s_dice2 = []\n",
    "s_hel2 = []\n",
    "s_jac2 = []\n",
    "s_jen2 = []\n",
    "s_cosine2 = []\n",
    "documents = [mat_s4,mark_s4,luke_s4,jam_s4,pet_s4,john_s4,jude_s4]\n",
    "for x in range(0,len(documents)):\n",
    "    s_dice2.append(dice(documents[x][0],documents[x][1]))\n",
    "    s_hel2.append(hellinger(documents[x][0],documents[x][1]))\n",
    "    s_jac2.append(jaccard(documents[x][0],documents[x][1]))\n",
    "    s_jen2.append(distance.jensenshannon(documents[x][0],documents[x][1]))\n",
    "    cos = tuple(cosine_similarity(documents[x])[0])\n",
    "    s_cosine2.append(cos[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "pqDbm-c0r-v9"
   },
   "outputs": [],
   "source": [
    "Tyndale2 = {'StanzaCosine':s_cosine2, 'StanzaDice':s_dice2, 'StanzaHellinger':s_hel2, 'StanzaJaccard':s_jac2, 'StanzaJSdivergence':s_jen2}\n",
    "Tyndale2 = pd.DataFrame(Tyndale2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "BuYrMDi8sG0v"
   },
   "outputs": [],
   "source": [
    "Tyndale_all = pd.concat([Tyndale, Tyndale1, Tyndale2], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "NWQULKQksRTO"
   },
   "outputs": [],
   "source": [
    "all_index = pd.concat([NA28_all,SBL_all,Tyndale_all], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "mi-UdNgWsWbF"
   },
   "outputs": [],
   "source": [
    "all_index.to_excel('Stanza_index.xlsx',index = False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Stanza.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
